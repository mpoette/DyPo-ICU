{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../params.json', 'r') as file :\n",
    "    params = json.load(file)\n",
    "\n",
    "DATASET, VERSION = params['dataset'], params['version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = f'/data2/poette.m/dypo/{VERSION}/3.analysis/times_series/{DATASET}/one_week.parquet'\n",
    "temporal_week = pl.read_parquet(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_week = temporal_week.drop('temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de la colonne total_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_col = list(set(temporal_week.columns) - set(['encounterId', 'intervalle']))\n",
    "\n",
    "temporal_week = temporal_week.with_columns(\n",
    "    pl.sum_horizontal(\n",
    "        [temporal_week[col].is_null() for col in features_col]\n",
    "    ).alias(\"total_missing\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération du dernier intervalle contenant des valeurs pour chaque patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1 : Identifier le dernier intervalle valide pour chaque patient\n",
    "max_valid_intervals = (\n",
    "    temporal_week.filter(pl.col(\"total_missing\") < 7)\n",
    "    .group_by(\"encounterId\")\n",
    "    .agg(pl.col(\"intervalle\").max().alias(\"max_valid_interval\"))\n",
    ")\n",
    "\n",
    "# Étape 2 : Supprimer les lignes inutiles\n",
    "cleaned_df = (\n",
    "    temporal_week.join(max_valid_intervals, on=\"encounterId\", how=\"inner\")\n",
    "    .filter(pl.col(\"intervalle\") <= pl.col(\"max_valid_interval\"))\n",
    "    .drop(\"max_valid_interval\")  # Supprimer la colonne temporaire\n",
    ")\n",
    "\n",
    "# Étape 3 : Trier les patients par leur intervalle max\n",
    "sorted_encounter_ids = (\n",
    "    max_valid_intervals.sort(\"max_valid_interval\", descending=True)\n",
    "    .with_row_index(name=\"sort_order\")\n",
    ")\n",
    "\n",
    "# Ajouter un ordre de tri directement via une jointure\n",
    "cleaned_df = (\n",
    "    cleaned_df.join(sorted_encounter_ids, on=\"encounterId\", how=\"left\")\n",
    "    #.filter(pl.col(\"max_valid_interval\") > 10)  # Supprimer les patients ayant un intervalle max faible\n",
    "    .sort([\"sort_order\", \"intervalle\"])\n",
    "    .drop(\"sort_order\")\n",
    "    .select(['encounterId', 'intervalle', 'heart_rate', 'spo2', 'fr', 'pad', 'pam', 'pas', 'total_missing', 'max_valid_interval'])\n",
    ")\n",
    "\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.n_unique('encounterId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création dataset des premières 48h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_48h = cleaned_df.filter(\n",
    "    (pl.col('max_valid_interval') > 47),\n",
    "    (pl.col('intervalle') < 48)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_48h.n_unique('encounterId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_48h = first_48h.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete encounters with too many missing values\n",
    "no_encounter = first_48h.columns.difference(['encounterId', 'intevralle'])\n",
    "first_48h_null = first_48h[no_encounter].isnull()\n",
    "first_48h_null['encounterId'] = first_48h['encounterId']\n",
    "first_48h_regroup = first_48h_null.groupby('encounterId').sum()\n",
    "theshold_missing = first_48h_regroup[first_48h_regroup > 14].dropna(axis = 0, how = 'all')\n",
    "encounters_with_missing = theshold_missing.reset_index()['encounterId'].to_list()\n",
    "print(len(encounters_with_missing))\n",
    "\n",
    "missing_first_48h =  first_48h[first_48h['encounterId'].isin(encounters_with_missing)]\n",
    "\n",
    "first_48h = first_48h[~first_48h.encounterId.isin(encounters_with_missing)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_48h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(first_48h[['fr', 'heart_rate', 'pam', 'pad', 'pas', 'spo2']].isna().sum(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_48h.encounterId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_48h.to_parquet(f'/data2/poette.m/dypo/{VERSION}/3.analysis/imputation_48/{DATASET}/first_48h.parquet')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
