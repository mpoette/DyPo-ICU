{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pypots.imputation import SAITS, BRITS, USGAN, GPVAE\n",
    "from pygrinder import mcar\n",
    "from pypots.utils.metrics import calc_mae, calc_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../params.json', 'r') as file :\n",
    "    params = json.load(file)\n",
    "\n",
    "DATASET, VERSION = params['dataset'], params['version']\n",
    "DATA_FOLD = params['data_folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = f'{DATA_FOLD}/{VERSION}/3.analysis/imputation_48/{DATASET}/'\n",
    "DATA_FILE = 'first_48h.parquet'\n",
    "MODEL_FOLDER = f'{DATA_FOLD}/{VERSION}/4.models/imputation/{DATASET}/'\n",
    "OUTPUT_TABLE = f'{DATA_FOLD}/{VERSION}/3.analysis/imputation_48/{DATASET}/tables/'\n",
    "OUTPUT_DATASET = f'{DATA_FOLD}/{VERSION}/3.analysis/imputation_48/{DATASET}/test_datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_48h = pl.read_parquet(DATA_FOLDER + DATA_FILE).to_pandas().drop(columns=['total_missing', 'max_valid_interval', '__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = first_48h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création des dataset de train/test/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartition des patients pour entraîner/test/validation\n",
    "patient_ids = data['encounterId'].unique()\n",
    "train_ids, test_val_ids = train_test_split(patient_ids, test_size=0.3, random_state=42)\n",
    "test_ids, val_ids = train_test_split(test_val_ids, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data['encounterId'].isin(train_ids)]\n",
    "test_data = data[data['encounterId'].isin(test_ids)]\n",
    "val_data = data[data['encounterId'].isin(val_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de l'absence de patient sans valeurs\n",
    "\n",
    "patients_with_missing_vars = (\n",
    "    test_data.groupby(\"encounterId\")\n",
    "    .apply(lambda group: group.drop(columns=[\"encounterId\", \"intervalle\"]).isnull().all(axis=0))\n",
    "    .any(axis=1)\n",
    ")\n",
    "\n",
    "# Filtrer les patients concernés\n",
    "patients_with_missing_vars = patients_with_missing_vars[patients_with_missing_vars].index.tolist()\n",
    "\n",
    "len(patients_with_missing_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)\n",
    "print(\"Validation shape:\", val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création des scénarios de données manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données manquantes aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mask(df, rate=0.3):\n",
    "    df_ori = df.copy()\n",
    "    ori_size = df_ori[~np.isnan(df_ori)].size\n",
    "    i=0\n",
    "    target_size = ori_size * (1-rate)\n",
    "    while (df[~np.isnan(df)].size  > target_size):\n",
    "        df = mcar(df, p=rate)\n",
    "        rate = rate / 2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toutes les données manquantes sur n timestamp consécutifs pour toute ou une partie des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_timestamp(array : np.ndarray, rate : float=0.3, n_timestamp : int=1, n_features : list = None) -> np.ndarray:\n",
    "    n_remove = int((array.shape[0] * array.shape[1] * rate)/n_timestamp)\n",
    "    \n",
    "    if n_features == None :\n",
    "        n_features = range(5)\n",
    "\n",
    "\n",
    "    for i in range(n_remove) :\n",
    "        intervalle_index = np.random.randint(0, array.shape[1])\n",
    "        encounter_index = np.random.randint(0, array.shape[0])\n",
    "        max_intervalle = intervalle_index+n_timestamp\n",
    "        if max_intervalle > array.shape[1]-1 :\n",
    "            max_intervalle = array.shape[1]-1\n",
    "            \n",
    "        array[encounter_index,intervalle_index:max_intervalle, n_features ] = np.nan\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepared_dataset(df, mask=None, rate : float=0.3, n_timestamp : int=1, n_features : list = None) :\n",
    "    \"\"\"\n",
    "    Prépare un dataset en appliquant diverses transformations :\n",
    "    - Standardisation\n",
    "    - Reshape en tableau 3D\n",
    "    - Application de masques spécifiques\n",
    "    - Vérification des proportions maximales de valeurs manquantes après masquage\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Données d'entrée avec colonnes 'encounterId' et 'intervalle'.\n",
    "        mask (function): Fonction de masquage à appliquer (facultatif).\n",
    "        rate (float): Taux minimal de valeurs manquantes autorisé par variable pour un patient.\n",
    "        n_timestamp (int): Nombre de timestamps à retirer si remove_timestamp est utilisé.\n",
    "        n_features (list): Liste des features à modifier si remove_timestamp est utilisé.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Tableau 3D transformé et éventuellement masqué.\n",
    "    \"\"\"\n",
    "    n_samples = df['encounterId'].unique().shape[0]\n",
    "    dropped_df = df.drop(['encounterId','intervalle'], axis=1)\n",
    "    standardized = StandardScaler().fit_transform(dropped_df.to_numpy())\n",
    "    reshaped = standardized.reshape(n_samples, 48, -1)\n",
    "\n",
    "    # Vérification des proportions maximales de valeurs manquantes\n",
    "\n",
    "    if mask == random_mask:\n",
    "        masked = random_mask(reshaped, rate)\n",
    "    elif mask == remove_timestamp :\n",
    "        masked = remove_timestamp(reshaped, rate, n_timestamp, n_features)\n",
    "    elif mask == None :\n",
    "        masked =  reshaped\n",
    "    else :\n",
    "        raise ValueError(\"Aucune correspondance concernant la fonction de masquage.\")\n",
    "\n",
    "    return masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = prepared_dataset(train_data)\n",
    "Train_mcar = prepared_dataset(train_data, mask=random_mask, rate=0.3)\n",
    "Val = prepared_dataset(val_data)\n",
    "Val_mcar = prepared_dataset(val_data, mask=random_mask, rate=0.3)\n",
    "Test_ori = prepared_dataset(test_data, mask=None)\n",
    "Test_mcar = prepared_dataset(test_data, mask=random_mask, rate=0.3)\n",
    "Test_single_row = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3) # missing intervalles\n",
    "Test_two_rows = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3, n_timestamp=2)\n",
    "Test_three_rows = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3, n_timestamp=3)\n",
    "Test_hr = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3, n_timestamp=4, n_features=[0])\n",
    "Test_sp02 = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3, n_timestamp=4, n_features=[1])\n",
    "Test_fr = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3, n_timestamp=4, n_features=[2])\n",
    "Test_pa = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3, n_timestamp=4, n_features=[3,4,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition des méthodes d'imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pypots models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"X\": Train,\n",
    "    \"missing_mask\": Train_mcar,\n",
    "    \"val_data\" : {\n",
    "        \"X\": Val,\n",
    "        \"missing_mask\": Val_mcar\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 48\n",
    "n_features = 6\n",
    "device = \"cuda\"\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.shape == Train_mcar.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/WenjieDu/SAITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits = SAITS(\n",
    "    n_steps=n_steps, n_features=n_features,\n",
    "    n_layers=3, d_model=512, d_ffn=128, n_heads=8, d_k=64, d_v=64,\n",
    "    dropout=0.1,\n",
    "    epochs=n_epochs,\n",
    "    device=device,\n",
    "    saving_path= MODEL_FOLDER + 'saits/model.pth',  # Stratégie de partage entre groupes\n",
    "    diagonal_attention_mask = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_saits = MODEL_FOLDER + \"saits/saits_two_days_with_val.pypots\"\n",
    "try :\n",
    "    saits.load(model_path_saits)\n",
    "except AssertionError :\n",
    "    print('model not found')\n",
    "    pass\n",
    "if fit_model :\n",
    "    saits.fit(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits.save(model_path_saits, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brits = BRITS(\n",
    "    n_steps=n_steps, \n",
    "    n_features=n_features, \n",
    "    rnn_hidden_size=128, \n",
    "    epochs=n_epochs, \n",
    "    device=device,\n",
    "    saving_path= MODEL_FOLDER + 'brits/model.pth'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_brits = MODEL_FOLDER + \"brits/brits_two_days_with_val.pypots\"\n",
    "try :\n",
    "    brits.load(model_path_brits)\n",
    "except AssertionError :\n",
    "    print('model not found')\n",
    "    pass\n",
    "if fit_model :\n",
    "    brits.fit(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brits.save(model_path_brits, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgan = USGAN(\n",
    "    n_steps=n_steps, \n",
    "    n_features=n_features, \n",
    "    epochs=n_epochs, \n",
    "    device=device, \n",
    "    rnn_hidden_size=128,\n",
    "    saving_path= MODEL_FOLDER + 'usgan/model.pth'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_usgan = MODEL_FOLDER + \"usgan/usgan_two_days_with_val.pypots\"\n",
    "try :\n",
    "    usgan.load(model_path_usgan)\n",
    "except AssertionError :\n",
    "    print('model not found')\n",
    "    pass\n",
    "if fit_model :\n",
    "    usgan.fit(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgan.save(model_path_usgan, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpvae = GPVAE(\n",
    "    n_steps=n_steps, \n",
    "    n_features=n_features, \n",
    "    epochs=n_epochs, \n",
    "    device=device, \n",
    "    latent_size=64,\n",
    "    saving_path= MODEL_FOLDER + 'gpvae/model.pth'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_gpvae = MODEL_FOLDER + \"gpvae/gpvae_two_days_with_val.pypots\"\n",
    "try :\n",
    "    gpvae.load(model_path_gpvae)\n",
    "except AssertionError :\n",
    "    print('model not found')\n",
    "    pass\n",
    "if fit_model :\n",
    "    gpvae.fit(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpvae.save(model_path_gpvae, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward/Backward Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(df):\n",
    "    \"\"\"\n",
    "    Applique un forward fill suivi d'un backward fill sur un tableau 3D numpy.\n",
    "    \n",
    "    Args:\n",
    "        data (numpy.ndarray): Tableau 3D (patients, timestamps, features) contenant des NaN.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Tableau avec les valeurs manquantes complétées.\n",
    "    \"\"\"\n",
    "    filled_data = np.copy(df)\n",
    "    series_no_values = 0\n",
    "    # Forward fill\n",
    "    for patient in range(filled_data.shape[0]):\n",
    "        for feature in range(filled_data.shape[2]):\n",
    "\n",
    "            pandas_df = pd.DataFrame(filled_data[patient, : , feature])\n",
    "            pandas_df = pandas_df.ffill().bfill()\n",
    "            filled_data[patient, :, feature] = pandas_df.values.flatten()\n",
    "\n",
    "    \n",
    "    return filled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_interpol(df):\n",
    "    \"\"\"\n",
    "    Complète les valeurs manquantes dans un tableau 3D numpy.\n",
    "    \n",
    "    1. Impute par la moyenne si des valeurs antérieures et ultérieures existent.\n",
    "    2. Forward fill si pas de données ultérieures.\n",
    "    3. Backward fill si pas de données antérieures.\n",
    "    \n",
    "    Args:\n",
    "        data (numpy.ndarray): Tableau 3D (patients, timestamps, features) contenant des NaN.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Tableau avec les valeurs manquantes complétées.\n",
    "    \"\"\"\n",
    "    filled_data = np.copy(df)\n",
    "\n",
    "    for patient in range(filled_data.shape[0]):\n",
    "        for feature in range(filled_data.shape[2]):\n",
    "            series = pd.Series(filled_data[patient, :, feature])\n",
    "\n",
    "            # Étape 1 : Imputation par la moyenne (si valeurs antérieures et ultérieures existent)\n",
    "            for idx in series[series.isna()].index:\n",
    "                # Chercher la dernière valeur antérieure\n",
    "                prev_idx = series[:idx].last_valid_index()\n",
    "                # Chercher la première valeur ultérieure\n",
    "                next_idx = series[idx + 1:].first_valid_index()\n",
    "                \n",
    "                if prev_idx is not None and next_idx is not None:\n",
    "                    prev_value = series[prev_idx]\n",
    "                    next_value = series[next_idx]\n",
    "                    series.iloc[idx] = (prev_value + next_value) / 2\n",
    "\n",
    "            # Étape 2 : Forward fill pour les NaN restants (pas de données ultérieures)\n",
    "            series.ffill(inplace=True)\n",
    "\n",
    "            # Étape 3 : Backward fill pour les NaN restants (pas de données antérieures)\n",
    "            series.bfill(inplace=True)\n",
    "\n",
    "            # Remplacer les données dans le tableau 3D\n",
    "            filled_data[patient, :, feature] = series.values\n",
    "\n",
    "    return filled_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation par la moyenne/médiane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_statistic(df, method=\"mean\"):\n",
    "    \"\"\"\n",
    "    Impute les valeurs manquantes pour chaque feature avec sa moyenne ou sa médiane.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): Tableau 3D (patients, timestamps, features) contenant des NaN.\n",
    "        method (str): Méthode d'imputation (\"mean\" ou \"median\").\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Tableau avec les valeurs manquantes imputées.\n",
    "    \"\"\"\n",
    "    filled_data = np.copy(df)\n",
    "    for patient in range(filled_data.shape[0]):\n",
    "        for feature in range(filled_data.shape[2]):\n",
    "            if method == \"mean\":\n",
    "                filled_value = np.nanmean(filled_data[patient, :, feature])\n",
    "            elif method == \"median\":\n",
    "                filled_value = np.nanmedian(filled_data[patient, :, feature])\n",
    "            else:\n",
    "                raise ValueError(\"Méthode non reconnue. Utilisez 'mean' ou 'median'.\")\n",
    "            \n",
    "            filled_data[:, :, feature] = np.nan_to_num(filled_data[:, :, feature], nan=filled_value)\n",
    "            \n",
    "    return filled_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/58613108/imputing-missing-values-using-sklearn-iterativeimputer-class-for-mice  \n",
    "https://github.com/wendyminai/APPROACHES-TO-MISSING-DATA-IN-TIME-SERIES-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputations 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten training data\n",
    "n_features = Train.shape[2]\n",
    "n_timestamps = Train.shape[1]\n",
    "train_samples = Train.shape[0]\n",
    "\n",
    "train_flatten = Train.reshape(-1, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation par MICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/58613108/imputing-missing-values-using-sklearn-iterativeimputer-class-for-mice  \n",
    "https://github.com/wendyminai/APPROACHES-TO-MISSING-DATA-IN-TIME-SERIES-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit IterativeImputer\n",
    "\n",
    "imputer_mice = IterativeImputer(max_iter=30, random_state=42)\n",
    "imputer_mice.fit(train_flatten)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation par KNNimputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_knn = KNNImputer(n_neighbors=2)\n",
    "imputer_knn.fit(train_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation par MissForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from missforest import MissForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_mf = MissForest()\n",
    "imputer_mf.fit(train_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute 3darray with 2d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_2d_model(df, model):\n",
    "    n_features = df.shape[2]\n",
    "    n_timestamps = df.shape[1]\n",
    "    n_samples = df.shape[0]\n",
    "\n",
    "    # Flatten data\n",
    "    flatten = pd.DataFrame(df.reshape(-1, n_features))\n",
    "\n",
    "    # Impute missing values\n",
    "    filled_flatten = model.transform(flatten)\n",
    "    if isinstance(filled_flatten, pd.DataFrame):\n",
    "        filled_flatten = filled_flatten.to_numpy()\n",
    "    # Reshape data\n",
    "    filled_data = filled_flatten.reshape(n_samples, n_timestamps, n_features)\n",
    "\n",
    "    return filled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditions valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [('Random' , Test_mcar), \n",
    "              ('Single_row' , Test_single_row), ('Two_rows', Test_two_rows), ('Three_rows', Test_three_rows), ('fr_only',Test_fr),\n",
    "              ('hr_only',Test_hr), ('pa_only',Test_pa),('spO2_only',Test_sp02)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_test = [('Random' , Test_mcar)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boucles méthodes d'imputation et scénarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mae = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_rmse = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Temps d'imputation de chaque modèle\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "\n",
    "for c in conditions :\n",
    "    time_cond = time.time()\n",
    "    print(f'----------------------------{c[0]}----------------------------')\n",
    "    imputation_saits = saits.impute({'X':c[1]})\n",
    "    saits_time = time.time() - time_cond\n",
    "    print(f'Saits time : {saits_time}')\n",
    "    imputation_brits = brits.impute({'X':c[1]})\n",
    "    brits_time = time.time() - saits_time\n",
    "    print(f'Brits time : {brits_time}')\n",
    "    imputation_usgan = usgan.impute({'X':c[1]})\n",
    "    usgan_time = time.time() - brits_time\n",
    "    print(f'Usgan time : {usgan_time}')\n",
    "    imputation_gpvae = gpvae.impute({'X':c[1]}).mean(axis=1)\n",
    "    gpvae_time = time.time() - usgan_time\n",
    "    print(f'Gpvae time : {gpvae_time}')\n",
    "    imputation_mice = impute_with_2d_model(c[1], imputer_mice)\n",
    "    mice_time = time.time() - gpvae_time\n",
    "    print(f'Mice time : {mice_time}')\n",
    "    imputation_mf = impute_with_2d_model(c[1], imputer_mf)\n",
    "    mf_time = time.time() - mice_time\n",
    "    print(f'Mf time : {mf_time}')\n",
    "    imputation_fill = fill_missing(c[1])\n",
    "    fill_time = time.time() - mf_time\n",
    "    print(f'Fill time : {fill_time}')\n",
    "    imputation_average_or_fill = lin_interpol(c[1])\n",
    "    average_time = time.time() - fill_time\n",
    "    print(f'Average time : {average_time}')\n",
    "    imputation_mean = impute_with_statistic(c[1])\n",
    "    mean_time = time.time() - average_time\n",
    "    print(f'Mean time : {mean_time}')\n",
    "    imputation_median = impute_with_statistic(c[1], method='median')\n",
    "    median_time = time.time() - mean_time\n",
    "    print(f'Median time : {median_time}')\n",
    "    imputed_datasets = [\n",
    "        ('fill', imputation_fill),\n",
    "        ('mean', imputation_mean),\n",
    "        ('median', imputation_median),\n",
    "        ('imputation_average', imputation_average_or_fill),\n",
    "        ('mice', imputation_mice),\n",
    "        ('mf', imputation_mf),\n",
    "        ('saits', imputation_saits),\n",
    "        ('brits', imputation_brits),\n",
    "        ('usgan', imputation_usgan),\n",
    "        ('gpvae', imputation_gpvae)\n",
    "    ]\n",
    "    print('imputation done')\n",
    "    full_imput_time = time.time() - time_cond\n",
    "    print(f'Imputation time : {full_imput_time}')\n",
    "    for i in imputed_datasets :\n",
    "        print(f'---------{i[0]}---------')\n",
    "        indicating_mask_test = np.isnan(c[1]) ^ np.isnan(Test_ori)\n",
    "        mae_test = calc_mae(i[1], np.nan_to_num(Test_ori), indicating_mask_test)\n",
    "        rmse_test = calc_rmse(i[1], np.nan_to_num(Test_ori), indicating_mask_test)\n",
    "        print(f'{c[0]} imputed with {i[0]} : MAE = {mae_test} / RMSE = {rmse_test}')\n",
    "        results_mae[i[0]][c[0]] = mae_test\n",
    "        results_rmse[i[0]][c[0]] = rmse_test\n",
    "results_df_mae = pd.DataFrame(results_mae)\n",
    "results_df_rmse = pd.DataFrame(results_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(results_df_mae.T, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(results_df_rmse.T, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fr_mae = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_hr_mae = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_pam_mae = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_pad_mae = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_pas_mae = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_sp02_mae = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fr_rmse = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_hr_rmse = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_pam_rmse = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_pad_rmse = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_pas_rmse = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_sp02_rmse = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_mae = [results_hr_mae,results_sp02_mae, results_fr_mae,  results_pad_mae, results_pam_mae, results_pas_mae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_rmse = [results_hr_rmse,results_sp02_rmse, results_fr_rmse,  results_pad_rmse, results_pam_rmse, results_pas_rmse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time_start = time.time()\n",
    "\n",
    "unscaled_df = test_data.drop(['encounterId','intervalle'], axis=1).to_numpy()\n",
    "scaler= StandardScaler().fit(unscaled_df)\n",
    "\n",
    "for c in conditions :\n",
    "\n",
    "    descaled_cond = scaler.inverse_transform(c[1].reshape(-1, n_features))\n",
    "\n",
    "    time_cond = time.time()\n",
    "    print(f'----------------------------{c[0]}----------------------------')\n",
    "    imputation_saits = saits.impute({'X':c[1]})\n",
    "    saits_time = time.time() - time_cond\n",
    "\n",
    "    imputation_brits = brits.impute({'X':c[1]})\n",
    "    brits_time = time.time() - saits_time\n",
    "\n",
    "    imputation_usgan = usgan.impute({'X':c[1]})\n",
    "    usgan_time = time.time() - brits_time\n",
    "\n",
    "    imputation_gpvae = gpvae.impute({'X':c[1]}).mean(axis=1)\n",
    "    gpvae_time = time.time() - usgan_time\n",
    "\n",
    "    imputation_mice = impute_with_2d_model(c[1], imputer_mice)\n",
    "    mice_time = time.time() - gpvae_time\n",
    " \n",
    "    imputation_mf = impute_with_2d_model(c[1], imputer_mf)\n",
    "    mf_time = time.time() - mice_time\n",
    "\n",
    "    imputation_fill = fill_missing(c[1])\n",
    "    fill_time = time.time() - mf_time\n",
    "\n",
    "    imputation_average_or_fill = lin_interpol(c[1])\n",
    "    average_time = time.time() - fill_time\n",
    " \n",
    "    imputation_mean = impute_with_statistic(c[1])\n",
    "    mean_time = time.time() - average_time\n",
    "    impute_with_statistic(c[1], method='median')\n",
    "    median_time = time.time() - mean_time\n",
    "\n",
    "    imputed_datasets = [\n",
    "        ('fill', imputation_fill),\n",
    "        ('mean', imputation_mean),\n",
    "        ('median', imputation_median),\n",
    "        ('imputation_average', imputation_average_or_fill),\n",
    "        ('mice', imputation_mice),\n",
    "        ('mf', imputation_mf),\n",
    "        ('saits', imputation_saits),\n",
    "        ('brits', imputation_brits),\n",
    "        ('usgan', imputation_usgan),\n",
    "        ('gpvae', imputation_gpvae)\n",
    "    ]\n",
    "    print('imputation done')\n",
    "    full_imput_time = time.time() - time_cond\n",
    "\n",
    "    for idx, feat in enumerate(df_features_mae) :\n",
    "\n",
    "        for i in imputed_datasets :\n",
    "            imputed_descaled = scaler.inverse_transform(i[1].reshape(-1, n_features))\n",
    "    \n",
    "            indicating_mask_test = np.isnan(descaled_cond[:,idx]) ^ np.isnan(unscaled_df[:,idx])\n",
    "            mae_test = calc_mae(imputed_descaled[:,idx], np.nan_to_num(unscaled_df[:,idx]), indicating_mask_test)\n",
    "            rmse_test = calc_rmse(imputed_descaled[:,idx], np.nan_to_num(unscaled_df[:,idx]), indicating_mask_test)\n",
    "\n",
    "            df_features_mae[idx][i[0]][c[0]] = mae_test\n",
    "            df_features_rmse[idx][i[0]][c[0]] = rmse_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_rmse[4]['saits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_mae[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index = ['heart_rate', 'spo2', 'fr', 'pad', 'pam', 'pas']\n",
    "for idx, i in enumerate(feature_index) :\n",
    "    print(i)\n",
    "    pd.DataFrame(df_features_mae[idx]).to_excel(OUTPUT_TABLE + f'mae_per_feature/feature_{i}_mae.xlsx')\n",
    "    pd.DataFrame(df_features_rmse[idx]).to_excel(OUTPUT_TABLE + f'rmse_per_feature/feature_{i}_rmse.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_mae.T.to_excel(OUTPUT_TABLE + 'results_global_imputation_mae.xlsx')\n",
    "results_df_rmse.T.to_excel(OUTPUT_TABLE + 'results_global_imputation_rmse.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute Dataset (SAITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id_intervalle = data[['encounterId', 'intervalle']]\n",
    "data_features = data[['fr', 'heart_rate', 'pam', 'pad', 'pas','spo2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshaped_inverse_scaler(original_dataset, imputed_dataset):\n",
    "\n",
    "    n_features = 6\n",
    "    n_timestamps = 48\n",
    "    n_samples = int(original_dataset.shape[0]/48)\n",
    "\n",
    "    data_id_intervalle = original_dataset[['encounterId', 'intervalle']]\n",
    "    data_features = original_dataset[['fr', 'heart_rate', 'pam', 'pad', 'pas','spo2']]\n",
    "    scaler = StandardScaler().fit(data_features.to_numpy())\n",
    "    data_imputed_reshaped = pd.DataFrame(scaler.inverse_transform(imputed_dataset.reshape(-1,6)), columns=['fr', 'heart_rate', 'pam', 'pad', 'pas','spo2'])\n",
    "    \n",
    "    return data_imputed_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = int(data.shape[0]/48)\n",
    "scaler = StandardScaler().fit(data_features.to_numpy())\n",
    "data_transformed = scaler.transform(data_features)\n",
    "data_reshaped = data_transformed.reshape(n_samples, 48, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed = saits.impute({'X':data_reshaped})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed_reshaped = pd.DataFrame(scaler.inverse_transform(data_imputed.reshape(-1,6)), columns=['fr', 'heart_rate', 'pam', 'pad', 'pas','spo2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed_reshaped.shape[0]/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed_reshaped['encounterId'] = data_id_intervalle['encounterId']\n",
    "data_imputed_reshaped['intervalle'] = data_id_intervalle['intervalle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id_intervalle['encounterId'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed_reshaped.fr.shape[0]/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed_reshaped = data_imputed_reshaped[['encounterId', 'intervalle', 'fr', 'heart_rate', 'spo2', 'pad', 'pam', 'pas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed_reshaped[data_imputed_reshaped['intervalle'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed_reshaped.to_parquet(OUTPUT_TABLE + 'first_48_imputed_saits.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de la répétabilité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_df = test_data.drop(['encounterId', 'intervalle'], axis=1).to_numpy()\n",
    "test_data.to_parquet(OUTPUT_DATASET + 'original.parquet')\n",
    "scaler = StandardScaler().fit(unscaled_df)\n",
    "\n",
    "# Étape 1: Filtrer les scénarios pour exclure ceux non pertinents\n",
    "filtered_conditions = [('pa_only',Test_pa)\n",
    "]\n",
    "\n",
    "# Étape 2: Préparer les datasets pour lin_interpol et SAITS\n",
    "datasets = {'lin_interpol': {}, 'saits': {}, 'mean' : {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario_name, scenario_data in filtered_conditions :\n",
    "    print(scenario_name)\n",
    "    descaled_cond = scaler.inverse_transform(scenario_data.reshape(-1, n_features))\n",
    "\n",
    "    imputation_saits = saits.impute({'X':scenario_data})\n",
    "\n",
    "    imputation_lin_interpol = lin_interpol(scenario_data)\n",
    "\n",
    "    imputation_moy = impute_with_statistic(scenario_data)\n",
    "\n",
    "    imputed_datasets = [\n",
    "        ('imputation_average', imputation_lin_interpol),\n",
    "        ('saits', imputation_saits),\n",
    "        ('mean', imputation_moy)\n",
    "\n",
    "    ]\n",
    "    print('imputation done')\n",
    "    \n",
    "    idx_pam = 4\n",
    "\n",
    "    indicating_mask_test = np.isnan(descaled_cond[:,idx_pam]) ^ np.isnan(unscaled_df[:,idx_pam])\n",
    "    original_values = unscaled_df[:,idx_pam][indicating_mask_test]\n",
    "    \n",
    "    saits_descaled = scaler.inverse_transform(imputation_saits.reshape(-1, n_features))\n",
    "    saits_pam = saits_descaled[:,idx_pam][indicating_mask_test]\n",
    "\n",
    "    datasets['saits'][scenario_name] = pd.DataFrame({\n",
    "        'masquées': original_values,  # Valeurs masquées (avec NaN)\n",
    "        'imputées': saits_pam   # Valeurs imputées par SAITS\n",
    "    })\n",
    "\n",
    "    lin_interpol_descaled = scaler.inverse_transform(imputation_lin_interpol.reshape(-1, n_features))\n",
    "    lin_interpol_pam = lin_interpol_descaled[:,idx_pam][indicating_mask_test]\n",
    "\n",
    "    datasets['lin_interpol'][scenario_name] = pd.DataFrame({\n",
    "        'masquées': original_values,  # Valeurs masquées (avec NaN)\n",
    "        'imputées': lin_interpol_pam   # Valeurs imputées par SAITS\n",
    "    })\n",
    "\n",
    "    mean_descaled = scaler.inverse_transform(imputation_moy.reshape(-1, n_features))\n",
    "    mean_pam = mean_descaled[:,idx_pam][indicating_mask_test]\n",
    "\n",
    "    datasets['mean'][scenario_name] = pd.DataFrame({\n",
    "        'masquées': original_values,  # Valeurs masquées (avec NaN)\n",
    "        'imputées': mean_pam   # Valeurs imputées par SAITS\n",
    "    })\n",
    "\n",
    "    descaled_cond = pd.DataFrame(descaled_cond, columns=['hr', 'spo2', 'fr', 'pad', 'pam', 'pas'])\n",
    "    descaled_cond['encounterId'] = test_data['encounterId']\n",
    "    descaled_cond['intervalle'] = test_data['intervalle']\n",
    "    descaled_cond.to_parquet(OUTPUT_DATASET + f'{scenario_name}_masked.parquet')\n",
    "\n",
    "    imputation_saits = pd.DataFrame(saits_descaled, columns=['hr', 'spo2', 'fr', 'pad', 'pam', 'pas'])\n",
    "    imputation_saits['encounterId'] = test_data['encounterId']\n",
    "    imputation_saits['intervalle'] = test_data['intervalle']\n",
    "    imputation_saits.to_parquet(OUTPUT_DATASET + f'{scenario_name}_imputed_saits.parquet')\n",
    "\n",
    "    imputation_lin_interpol = pd.DataFrame(lin_interpol_descaled, columns=['hr', 'spo2', 'fr', 'pad', 'pam', 'pas'])\n",
    "    imputation_lin_interpol['encounterId'] = test_data['encounterId']\n",
    "    imputation_lin_interpol['intervalle'] = test_data['intervalle']\n",
    "    imputation_lin_interpol.to_parquet(OUTPUT_DATASET + f'{scenario_name}_imputed_lin_interpol.parquet')\n",
    "\n",
    "    imputation_moy = pd.DataFrame(mean_descaled, columns=['hr', 'spo2', 'fr', 'pad', 'pam', 'pas'])\n",
    "    imputation_moy['encounterId'] = test_data['encounterId']\n",
    "    imputation_moy['intervalle'] = test_data['intervalle']\n",
    "    imputation_moy.to_parquet(OUTPUT_DATASET + f'{scenario_name}_imputed_mean.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 3: Calculer les moyennes et différences\n",
    "for method, scenario_data in datasets.items():\n",
    "    for scenario_name, df in scenario_data.items():\n",
    "        df['moyenne'] = (df['masquées'] + df['imputées']) / 2\n",
    "        df['différence'] = df['imputées'] - df['masquées']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = OUTPUT_TABLE + 'pam_comparaison/'\n",
    "\n",
    "for method, scenario_data in datasets.items():\n",
    "    for scenario_name, df in scenario_data.items():\n",
    "        filtered_df = df.dropna(subset=['masquées'])\n",
    "        file_name = f\"{scenario_name}_{method}.xlsx\"\n",
    "        filtered_df.to_excel(output_dir + file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fonction combinée pour tracer les deux graphiques avec un titre commun\n",
    "def plot_combined_graphs(df, method, scenario_name):\n",
    "    # Créer une figure avec deux sous-graphiques côte à côte\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Ajouter un titre commun à la figure\n",
    "    fig.suptitle(f'{method} - {scenario_name}', fontsize=16, y=1.02)\n",
    "\n",
    "    # Sous-graphe 1 : Bland et Altman\n",
    "    axes[0].scatter(df['masquées'], df['différence'], alpha=0.5, label='Points')\n",
    "    \n",
    "    mean_diff = np.mean(df['différence'])\n",
    "    std_diff = np.std(df['différence'])\n",
    "    upper_limit = mean_diff + 1.96 * std_diff\n",
    "    lower_limit = mean_diff - 1.96 * std_diff\n",
    "    \n",
    "    axes[0].axhline(mean_diff, color='red', linestyle='--', label=f'Moyenne des différences ({mean_diff:.2f})')\n",
    "    axes[0].axhline(upper_limit, color='blue', linestyle='--', label=f'Limite supérieure ({upper_limit:.2f})')\n",
    "    axes[0].axhline(lower_limit, color='blue', linestyle='--', label=f'Limite inférieure ({lower_limit:.2f})')\n",
    "    \n",
    "    axes[0].set_title('Bland et Altman')\n",
    "    axes[0].set_xlabel('Moyenne des valeurs (masquées et imputées)')\n",
    "    axes[0].set_ylabel('Différence (imputées - masquées)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Sous-graphe 2 : Valeurs imputées en fonction des valeurs masquées\n",
    "    axes[1].scatter(df['masquées'], df['imputées'], alpha=0.5, label='Points')\n",
    "    \n",
    "    axes[1].set_title('Valeurs imputées vs masquées')\n",
    "    axes[1].set_xlabel('Valeurs masquées')\n",
    "    axes[1].set_ylabel('Valeurs imputées')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # Ajuster l'espacement entre les graphiques et le titre commun\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method, scenario_data in datasets.items():\n",
    "    for scenario_name, df in scenario_data.items():\n",
    "        plot_combined_graphs(df, method, scenario_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATASET)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
