{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pypots.imputation import SAITS, BRITS, USGAN, GPVAE\n",
    "from pygrinder import mcar\n",
    "from pypots.utils.metrics import calc_mae, calc_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../params.json', 'r') as file :\n",
    "    params = json.load(file)\n",
    "\n",
    "DATASET, VERSION = params['dataset'], params['version']\n",
    "DATA_FOLD = params['data_folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = f'{DATA_FOLD}/{VERSION}/3.analysis/imputation_48/{DATASET}/'\n",
    "DATA_FILE = 'first_48h_with_static.parquet'\n",
    "MODEL_FOLDER = f'{DATA_FOLD}/{VERSION}/4.models/imputation/{DATASET}/'\n",
    "OUTPUT_TABLE = f'{DATA_FOLD}/{VERSION}/3.analysis/imputation_48/{DATASET}/tables/'\n",
    "OUTPUT_DATASET = f'{DATA_FOLD}/{VERSION}/3.analysis/imputation_48/{DATASET}/test_datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_48h = pl.read_parquet(DATA_FOLDER + DATA_FILE).to_pandas().drop(columns=['total_missing', 'max_valid_interval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = first_48h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cr√©ation des dataset de train/test/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©partition des patients pour entra√Æner/test/validation\n",
    "patient_ids = data['encounterId'].unique()\n",
    "train_ids, test_val_ids = train_test_split(patient_ids, test_size=0.3, random_state=42)\n",
    "test_ids, val_ids = train_test_split(test_val_ids, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data['encounterId'].isin(train_ids)]\n",
    "test_data = data[data['encounterId'].isin(test_ids)]\n",
    "val_data = data[data['encounterId'].isin(val_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification de l'absence de patient sans valeurs\n",
    "\n",
    "patients_with_missing_vars = (\n",
    "    test_data.groupby(\"encounterId\")\n",
    "    .apply(lambda group: group.drop(columns=[\"encounterId\", \"intervalle\"]).isnull().all(axis=0))\n",
    "    .any(axis=1)\n",
    ")\n",
    "\n",
    "# Filtrer les patients concern√©s\n",
    "patients_with_missing_vars = patients_with_missing_vars[patients_with_missing_vars].index.tolist()\n",
    "\n",
    "len(patients_with_missing_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)\n",
    "print(\"Validation shape:\", val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cr√©ation des sc√©narios de donn√©es manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donn√©es manquantes al√©atoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mask(df, rate=0.3, idx_features = 4):\n",
    "    df_ori = df.copy()\n",
    "    df_dyn = df[:,:,:idx_features]\n",
    "    df_ori_dyn = df_ori[:,:,:idx_features]\n",
    "    ori_size = df_ori_dyn[~np.isnan(df_ori_dyn)].size\n",
    "\n",
    "    target_size = ori_size * (1-rate)\n",
    "    while (df[:,:,:idx_features][~np.isnan(df[:,:,:idx_features])].size  > target_size):\n",
    "        df[:,:,:idx_features] = mcar(df[:,:,:idx_features], p=rate)\n",
    "        rate = rate / 2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toutes les donn√©es manquantes sur n timestamp cons√©cutifs pour toute ou une partie des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_timestamp(array : np.ndarray, rate : float=0.3, n_timestamp : int=1, n_features : list = list(range(4))) -> np.ndarray:\n",
    "    n_remove = int((array.shape[0] * array.shape[1] * rate)/n_timestamp)\n",
    "\n",
    "\n",
    "    for i in range(n_remove) :\n",
    "        intervalle_index = np.random.randint(0, array.shape[1])\n",
    "        encounter_index = np.random.randint(0, array.shape[0])\n",
    "        max_intervalle = intervalle_index+n_timestamp\n",
    "        if max_intervalle > array.shape[1]-1 :\n",
    "            max_intervalle = array.shape[1]-1\n",
    "            \n",
    "        array[encounter_index,intervalle_index:max_intervalle, n_features ] = np.nan\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©paration du jeu de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepared_dataset(df, mask=None, rate : float=0.3, n_timestamp : int=1, n_features : list = list(range(4))) :\n",
    "    \"\"\"\n",
    "    Pr√©pare un dataset en appliquant diverses transformations :\n",
    "    - Standardisation\n",
    "    - Reshape en tableau 3D\n",
    "    - Application de masques sp√©cifiques\n",
    "    - V√©rification des proportions maximales de valeurs manquantes apr√®s masquage\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Donn√©es d'entr√©e avec colonnes 'encounterId' et 'intervalle'.\n",
    "        mask (function): Fonction de masquage √† appliquer (facultatif).\n",
    "        rate (float): Taux minimal de valeurs manquantes autoris√© par variable pour un patient.\n",
    "        n_timestamp (int): Nombre de timestamps √† retirer si remove_timestamp est utilis√©.\n",
    "        n_features (list): Liste des features √† modifier si remove_timestamp est utilis√©.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Tableau 3D transform√© et √©ventuellement masqu√©.\n",
    "    \"\"\"\n",
    "    n_samples = df['encounterId'].unique().shape[0]\n",
    "    dropped_df = df.drop(['encounterId','intervalle'], axis=1)\n",
    "    standardized = StandardScaler().fit_transform(dropped_df.to_numpy())\n",
    "    reshaped = standardized.reshape(n_samples, 48, -1)\n",
    "\n",
    "    # V√©rification des proportions maximales de valeurs manquantes\n",
    "\n",
    "    if mask == random_mask:\n",
    "        masked = random_mask(reshaped, rate)\n",
    "    elif mask == remove_timestamp :\n",
    "        masked = remove_timestamp(reshaped, rate, n_timestamp, n_features)\n",
    "    elif mask == None :\n",
    "        masked =  reshaped\n",
    "    else :\n",
    "        raise ValueError(\"Aucune correspondance concernant la fonction de masquage.\")\n",
    "\n",
    "    return masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = prepared_dataset(train_data)\n",
    "Train_mcar = prepared_dataset(train_data, mask=random_mask, rate=0.3)\n",
    "Val = prepared_dataset(val_data)\n",
    "Val_mcar = prepared_dataset(val_data, mask=random_mask, rate=0.3)\n",
    "Test_ori = prepared_dataset(test_data, mask=None)\n",
    "Test_mcar = prepared_dataset(test_data, mask=random_mask, rate=0.3)\n",
    "Test_single_row = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3) # missing intervalles\n",
    "Test_two_rows = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3, n_timestamp=2)\n",
    "Test_three_rows = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3, n_timestamp=3)\n",
    "Test_hr = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3, n_timestamp=4, n_features=[0])\n",
    "Test_sp02 = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3, n_timestamp=4, n_features=[1])\n",
    "Test_fr = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3, n_timestamp=4, n_features=[2])\n",
    "Test_pa = prepared_dataset(test_data, mask=remove_timestamp, rate=0.3, n_timestamp=4, n_features=[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D√©finition des m√©thodes d'imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pypots models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"X\": Train,\n",
    "    \"missing_mask\": Train_mcar,\n",
    "    \"val_data\" : {\n",
    "        \"X\": Val,\n",
    "        \"missing_mask\": Val_mcar\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 48\n",
    "n_features = 7\n",
    "device = \"cuda\"\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.shape == Train_mcar.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/WenjieDu/SAITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits = SAITS(\n",
    "    n_steps=n_steps, n_features=n_features,\n",
    "    n_layers=3, d_model=512, d_ffn=128, n_heads=8, d_k=64, d_v=64,\n",
    "    dropout=0.1,\n",
    "    epochs=n_epochs,\n",
    "    device=device,\n",
    "    saving_path= MODEL_FOLDER + 'saits/model.pth',  # Strat√©gie de partage entre groupes\n",
    "    diagonal_attention_mask = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {\n",
    "    'd_model' : [64,128,256,512], \n",
    "     'd_ffn' : [128,256,512,1024], \n",
    "     'n_heads' : [2,4,8],\n",
    "     'd_k' : [32, 64, 128, 256],\n",
    "     'd_v' : [32, 64, 128, 256],\n",
    "    'dropout' : [0, 0.1, 0.2, 0.3, 0.4, 0.5]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_saits = MODEL_FOLDER + \"saits/saits_two_days_with_val.pypots\"\n",
    "try :\n",
    "    saits.load(model_path_saits)\n",
    "except AssertionError :\n",
    "    print('model not found')\n",
    "    pass\n",
    "except RuntimeError :\n",
    "    pass\n",
    "if fit_model :\n",
    "    saits.fit(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits.save(model_path_saits, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brits = BRITS(\n",
    "    n_steps=n_steps, \n",
    "    n_features=n_features, \n",
    "    rnn_hidden_size=128, \n",
    "    epochs=n_epochs, \n",
    "    device=device,\n",
    "    saving_path= MODEL_FOLDER + 'brits/model.pth'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_brits = MODEL_FOLDER + \"brits/brits_two_days_with_val.pypots\"\n",
    "try :\n",
    "    brits.load(model_path_brits)\n",
    "except AssertionError :\n",
    "    print('model not found')\n",
    "    pass\n",
    "except RuntimeError :\n",
    "    pass\n",
    "if fit_model :\n",
    "    brits.fit(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brits.save(model_path_brits, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgan = USGAN(\n",
    "    n_steps=n_steps, \n",
    "    n_features=n_features, \n",
    "    epochs=n_epochs, \n",
    "    device=device, \n",
    "    rnn_hidden_size=128,\n",
    "    saving_path= MODEL_FOLDER + 'usgan/model.pth'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_usgan = MODEL_FOLDER + \"usgan/usgan_two_days_with_val.pypots\"\n",
    "try :\n",
    "    usgan.load(model_path_usgan)\n",
    "except AssertionError :\n",
    "    print('model not found')\n",
    "    pass\n",
    "except RuntimeError :\n",
    "    pass\n",
    "if fit_model :\n",
    "    usgan.fit(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgan.save(model_path_usgan, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpvae = GPVAE(\n",
    "    n_steps=n_steps, \n",
    "    n_features=n_features, \n",
    "    epochs=n_epochs, \n",
    "    device=device, \n",
    "    latent_size=64,\n",
    "    saving_path= MODEL_FOLDER + 'gpvae/model.pth'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_gpvae = MODEL_FOLDER + \"gpvae/gpvae_two_days_with_val.pypots\"\n",
    "try :\n",
    "    gpvae.load(model_path_gpvae)\n",
    "except AssertionError :\n",
    "    print('model not found')\n",
    "    pass\n",
    "except RuntimeError :\n",
    "    pass\n",
    "if fit_model :\n",
    "    gpvae.fit(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpvae.save(model_path_gpvae, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward/Backward Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(df):\n",
    "    \"\"\"\n",
    "    Applique un forward fill suivi d'un backward fill sur un tableau 3D numpy.\n",
    "    \n",
    "    Args:\n",
    "        data (numpy.ndarray): Tableau 3D (patients, timestamps, features) contenant des NaN.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Tableau avec les valeurs manquantes compl√©t√©es.\n",
    "    \"\"\"\n",
    "    filled_data = np.copy(df)\n",
    "    series_no_values = 0\n",
    "    # Forward fill\n",
    "    for patient in range(filled_data.shape[0]):\n",
    "        for feature in range(filled_data.shape[2]):\n",
    "\n",
    "            pandas_df = pd.DataFrame(filled_data[patient, : , feature])\n",
    "            pandas_df = pandas_df.ffill().bfill()\n",
    "            filled_data[patient, :, feature] = pandas_df.values.flatten()\n",
    "\n",
    "    \n",
    "    return filled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation lin√©aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_interpol(df):\n",
    "    \"\"\"\n",
    "    Compl√®te les valeurs manquantes dans un tableau 3D numpy.\n",
    "    \n",
    "    1. Impute par la moyenne si des valeurs ant√©rieures et ult√©rieures existent.\n",
    "    2. Forward fill si pas de donn√©es ult√©rieures.\n",
    "    3. Backward fill si pas de donn√©es ant√©rieures.\n",
    "    \n",
    "    Args:\n",
    "        data (numpy.ndarray): Tableau 3D (patients, timestamps, features) contenant des NaN.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Tableau avec les valeurs manquantes compl√©t√©es.\n",
    "    \"\"\"\n",
    "    filled_data = np.copy(df)\n",
    "\n",
    "    for patient in range(filled_data.shape[0]):\n",
    "        for feature in range(filled_data.shape[2]):\n",
    "            series = pd.Series(filled_data[patient, :, feature])\n",
    "\n",
    "            series.interpolate(method='linear', inplace=True, limit_direction='both')\n",
    "            # √âtape 1 : Imputation par la moyenne (si valeurs ant√©rieures et ult√©rieures existent)\n",
    "            \"\"\"for idx in series[series.isna()].index:\n",
    "                # Chercher la derni√®re valeur ant√©rieure\n",
    "                prev_idx = series[:idx].last_valid_index()\n",
    "                # Chercher la premi√®re valeur ult√©rieure\n",
    "                next_idx = series[idx + 1:].first_valid_index()\n",
    "                \n",
    "                if prev_idx is not None and next_idx is not None:\n",
    "                    prev_value = series[prev_idx]\n",
    "                    next_value = series[next_idx]\n",
    "                    series.iloc[idx] = (prev_value + next_value) / 2\n",
    "            \"\"\"\n",
    "            # √âtape 2 : Forward fill pour les NaN restants (pas de donn√©es ult√©rieures)\n",
    "            series.ffill(inplace=True)\n",
    "\n",
    "            # √âtape 3 : Backward fill pour les NaN restants (pas de donn√©es ant√©rieures)\n",
    "            series.bfill(inplace=True)\n",
    "\n",
    "            # Remplacer les donn√©es dans le tableau 3D\n",
    "            filled_data[patient, :, feature] = series.values\n",
    "\n",
    "    return filled_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation par la moyenne/m√©diane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_statistic(df, method=\"mean\"):\n",
    "    \"\"\"\n",
    "    Impute les valeurs manquantes pour chaque feature avec sa moyenne ou sa m√©diane.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): Tableau 3D (patients, timestamps, features) contenant des NaN.\n",
    "        method (str): M√©thode d'imputation (\"mean\" ou \"median\").\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Tableau avec les valeurs manquantes imput√©es.\n",
    "    \"\"\"\n",
    "    filled_data = np.copy(df)\n",
    "    for patient in range(filled_data.shape[0]):\n",
    "        for feature in range(filled_data.shape[2]):\n",
    "            if method == \"mean\":\n",
    "                filled_value = np.nanmean(filled_data[patient, :, feature])\n",
    "            elif method == \"median\":\n",
    "                filled_value = np.nanmedian(filled_data[patient, :, feature])\n",
    "            else:\n",
    "                raise ValueError(\"M√©thode non reconnue. Utilisez 'mean' ou 'median'.\")\n",
    "            \n",
    "            filled_data[:, :, feature] = np.nan_to_num(filled_data[:, :, feature], nan=filled_value)\n",
    "            \n",
    "    return filled_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/58613108/imputing-missing-values-using-sklearn-iterativeimputer-class-for-mice  \n",
    "https://github.com/wendyminai/APPROACHES-TO-MISSING-DATA-IN-TIME-SERIES-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputations 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten training data\n",
    "n_features = Train.shape[2]\n",
    "n_timestamps = Train.shape[1]\n",
    "train_samples = Train.shape[0]\n",
    "\n",
    "train_flatten = Train.reshape(-1, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation par MICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/58613108/imputing-missing-values-using-sklearn-iterativeimputer-class-for-mice  \n",
    "https://github.com/wendyminai/APPROACHES-TO-MISSING-DATA-IN-TIME-SERIES-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit IterativeImputer\n",
    "\n",
    "imputer_mice = IterativeImputer(max_iter=30, random_state=42)\n",
    "imputer_mice.fit(train_flatten)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation par KNNimputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_knn = KNNImputer(n_neighbors=2)\n",
    "imputer_knn.fit(train_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation par MissForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from missforest import MissForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_mf = MissForest()\n",
    "imputer_mf.fit(train_flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute 3darray with 2d model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_2d_model(df, model):\n",
    "    n_features = df.shape[2]\n",
    "    n_timestamps = df.shape[1]\n",
    "    n_samples = df.shape[0]\n",
    "\n",
    "    # Flatten data\n",
    "    flatten = pd.DataFrame(df.reshape(-1, n_features))\n",
    "\n",
    "    # Impute missing values\n",
    "    filled_flatten = model.transform(flatten)\n",
    "    if isinstance(filled_flatten, pd.DataFrame):\n",
    "        filled_flatten = filled_flatten.to_numpy()\n",
    "    # Reshape data\n",
    "    filled_data = filled_flatten.reshape(n_samples, n_timestamps, n_features)\n",
    "\n",
    "    return filled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditions valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [('Random' , Test_mcar), \n",
    "              ('Single_row' , Test_single_row), ('Two_rows', Test_two_rows), ('Three_rows', Test_three_rows), ('fr_only',Test_fr),\n",
    "              ('hr_only',Test_hr), ('pa_only',Test_pa),('spO2_only',Test_sp02)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_test = [('Random' , Test_mcar)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boucles m√©thodes d'imputation et sc√©narios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mae = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf',  'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_rmse = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test avec SAITS uniquement en NN\n",
    "time_start = time.time()\n",
    "\n",
    "for c in conditions:\n",
    "    time_cond = time.time()\n",
    "    print(f'----------------------------{c[0]}----------------------------')\n",
    "\n",
    "    # SAITS\n",
    "    imputation_saits = saits.impute({'X': c[1]})\n",
    "    saits_time = time.time() - time_cond\n",
    "    print(f'Saits time : {saits_time:.3f} sec')\n",
    "\n",
    "    # BRITS\n",
    "    time_brits_start = time.time()\n",
    "    imputation_brits = brits.impute({'X':c[1]})\n",
    "    brits_time = time.time() - time_brits_start\n",
    "    print(f'BRITS time : {brits_time:.3f} sec')\n",
    "\n",
    "    # USGAN\n",
    "    time_usgan_start = time.time()\n",
    "    imputation_usgan = usgan.impute({'X':c[1]})\n",
    "    usgan_time = time.time() - time_usgan_start\n",
    "    print(f'Usgan time : {usgan_time:.3f} sec')\n",
    "\n",
    "    # GPVAE\n",
    "    time_gpvae_start = time.time()\n",
    "    imputation_gpvae = gpvae.impute({'X':c[1]}).mean(axis=1)\n",
    "    gpvae_time = time.time() - time_gpvae_start\n",
    "    print(f'GPVAE time : {gpvae_time:.3f} sec')\n",
    "\n",
    "    # MICE\n",
    "    time_mice_start = time.time()\n",
    "    imputation_mice = impute_with_2d_model(c[1], imputer_mice)  # Remplace avec ta fonction MICE\n",
    "    mice_time = time.time() - time_mice_start\n",
    "    print(f'Mice time : {mice_time:.3f} sec')\n",
    "\n",
    "    # MF\n",
    "    time_mf_start = time.time()\n",
    "    imputation_mf = impute_with_2d_model(c[1], imputer_mf)\n",
    "    mf_time = time.time() - time_mf_start\n",
    "    print(f'MF time : {mf_time:.3f} sec')\n",
    "\n",
    "    # Fill missing\n",
    "    time_fill_start = time.time()\n",
    "    imputation_fill = fill_missing(c[1])\n",
    "    fill_time = time.time() - time_fill_start\n",
    "    print(f'Fill time : {fill_time:.3f} sec')\n",
    "\n",
    "    # Linear interpolation\n",
    "    time_average_start = time.time()\n",
    "    imputation_average_or_fill = lin_interpol(c[1])\n",
    "    average_time = time.time() - time_average_start\n",
    "    print(f'Lin interpol time : {average_time:.3f} sec')\n",
    "\n",
    "    # Mean imputation\n",
    "    time_mean_start = time.time()\n",
    "    imputation_mean = impute_with_statistic(c[1])\n",
    "    mean_time = time.time() - time_mean_start\n",
    "    print(f'Mean time : {mean_time:.3f} sec')\n",
    "\n",
    "    # Median imputation\n",
    "    time_median_start = time.time()\n",
    "    imputation_median = impute_with_statistic(c[1], method='median')\n",
    "    median_time = time.time() - time_median_start\n",
    "    print(f'Median time : {median_time:.3f} sec')\n",
    "\n",
    "    # Temps total\n",
    "    total_time = time.time() - time_start\n",
    "    print(f'\\nTemps total : {total_time:.3f} sec ({time.strftime(\"%M:%S\", time.gmtime(total_time))})')\n",
    "\n",
    "    imputed_datasets = [\n",
    "        ('fill', imputation_fill),\n",
    "        ('mean', imputation_mean),\n",
    "        ('median', imputation_median),\n",
    "        ('imputation_average', imputation_average_or_fill),\n",
    "        ('mice', imputation_mice),\n",
    "        ('mf', imputation_mf),\n",
    "        ('saits', imputation_saits),\n",
    "        ('brits', imputation_brits),\n",
    "        ('usgan', imputation_usgan),\n",
    "        ('gpvae', imputation_gpvae)\n",
    "    ]\n",
    "    print('imputation done')\n",
    "    full_imput_time = time.time() - time_cond\n",
    "    print(f'Imputation time : {full_imput_time}')\n",
    "    for i in imputed_datasets :\n",
    "        print(f'---------{i[0]}---------')\n",
    "        indicating_mask_test = np.isnan(c[1]) ^ np.isnan(Test_ori)\n",
    "        mae_test = calc_mae(i[1], np.nan_to_num(Test_ori), indicating_mask_test)\n",
    "        rmse_test = calc_rmse(i[1], np.nan_to_num(Test_ori), indicating_mask_test)\n",
    "        print(f'{c[0]} imputed with {i[0]} : MAE = {mae_test} / RMSE = {rmse_test}')\n",
    "        results_mae[i[0]][c[0]] = mae_test\n",
    "        results_rmse[i[0]][c[0]] = rmse_test\n",
    "results_df_mae = pd.DataFrame(results_mae)\n",
    "results_df_rmse = pd.DataFrame(results_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(results_df_mae.T, 3).to_excel(OUTPUT_TABLE + f'mae_{DATASET}.xlsx')\n",
    "round(results_df_mae.T, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(results_df_rmse.T, 3).to_excel(OUTPUT_TABLE + f'rmse_{DATASET}.xlsx')\n",
    "round(results_df_rmse.T, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fr_mae = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_hr_mae = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_pam_mae = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_sp02_mae = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fr_rmse = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_hr_rmse = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_pam_rmse = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}\n",
    "results_sp02_rmse = {method: {} for method in ['mean', 'median', 'imputation_average', 'fill', 'mice', 'mf', 'saits', 'brits', 'usgan', 'gpvae']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_mae = [results_hr_mae, results_sp02_mae, results_fr_mae, results_pam_mae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_rmse = [results_hr_rmse, results_sp02_rmse, results_fr_rmse, results_pam_rmse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unscaled_df = test_data.drop(['encounterId','intervalle'], axis=1).to_numpy()\n",
    "scaler= StandardScaler().fit(unscaled_df)\n",
    "\n",
    "for c in conditions :\n",
    "\n",
    "    print(f'----------------------------{c[0]}----------------------------')\n",
    "    imputation_saits = saits.impute({'X':c[1]})\n",
    "\n",
    "    imputation_brits = brits.impute({'X':c[1]})\n",
    "\n",
    "    imputation_usgan = usgan.impute({'X':c[1]})\n",
    "\n",
    "    imputation_gpvae = gpvae.impute({'X':c[1]}).mean(axis=1)\n",
    "\n",
    "    imputation_mice = impute_with_2d_model(c[1], imputer_mice)\n",
    " \n",
    "    imputation_mf = impute_with_2d_model(c[1], imputer_mf)\n",
    "\n",
    "    imputation_fill = fill_missing(c[1])\n",
    "\n",
    "    imputation_average_or_fill = lin_interpol(c[1])\n",
    " \n",
    "    imputation_mean = impute_with_statistic(c[1])\n",
    "\n",
    "    imputation_median = impute_with_statistic(c[1], method='median')\n",
    "\n",
    "\n",
    "    imputed_datasets = [\n",
    "        ('fill', imputation_fill),\n",
    "        ('mean', imputation_mean),\n",
    "        ('median', imputation_median),\n",
    "        ('imputation_average', imputation_average_or_fill),\n",
    "        ('mice', imputation_mice),\n",
    "        ('mf', imputation_mf),\n",
    "        ('saits', imputation_saits),\n",
    "        ('brits', imputation_brits),\n",
    "        ('usgan', imputation_usgan),\n",
    "        ('gpvae', imputation_gpvae)\n",
    "    ]\n",
    "    print('imputation done')\n",
    "\n",
    "    descaled_cond = scaler.inverse_transform(c[1].reshape(-1, n_features))\n",
    "    \n",
    "    for idx, feat in enumerate(df_features_mae) :\n",
    "\n",
    "        for i in imputed_datasets :\n",
    "            imputed_descaled = scaler.inverse_transform(i[1].reshape(-1, n_features))\n",
    "    \n",
    "            indicating_mask_test = np.isnan(descaled_cond[:,idx]) ^ np.isnan(unscaled_df[:,idx])\n",
    "            mae_test = calc_mae(imputed_descaled[:,idx], np.nan_to_num(unscaled_df[:,idx]), indicating_mask_test)\n",
    "            rmse_test = calc_rmse(imputed_descaled[:,idx], np.nan_to_num(unscaled_df[:,idx]), indicating_mask_test)\n",
    "\n",
    "            df_features_mae[idx][i[0]][c[0]] = mae_test\n",
    "            df_features_rmse[idx][i[0]][c[0]] = rmse_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index = ['heart_rate', 'spo2', 'fr', 'pam']\n",
    "for idx, i in enumerate(feature_index) :\n",
    "    print(i)\n",
    "    pd.DataFrame(df_features_mae[idx]).T.to_excel(OUTPUT_TABLE + f'mae_per_feature/feature_{i}_mae.xlsx')\n",
    "    pd.DataFrame(df_features_rmse[idx]).T.to_excel(OUTPUT_TABLE + f'rmse_per_feature/feature_{i}_rmse.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_mae.T.to_excel(OUTPUT_TABLE + 'results_global_imputation_mae.xlsx')\n",
    "results_df_rmse.T.to_excel(OUTPUT_TABLE + 'results_global_imputation_rmse.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute Dataset (SAITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id_intervalle = data[['encounterId', 'intervalle']]\n",
    "data_features = data.drop(columns=data_id_intervalle.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshaped_inverse_scaler(original_dataset, imputed_dataset):\n",
    "\n",
    "\n",
    "    data_id_intervalle = original_dataset[['encounterId', 'intervalle']]\n",
    "    data_features = original_dataset.drop(columns=data_id_intervalle.columns)\n",
    "    scaler = StandardScaler().fit(data_features.to_numpy())\n",
    "    data_imputed_reshaped = pd.DataFrame(scaler.inverse_transform(imputed_dataset.reshape(-1,6)), columns=data_features.columns)\n",
    "    \n",
    "    return data_imputed_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = int(data.shape[0]/48)\n",
    "scaler = StandardScaler().fit(data_features.to_numpy())\n",
    "data_transformed = scaler.transform(data_features)\n",
    "data_reshaped = data_transformed.reshape(n_samples, 48, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed = saits.impute({'X':data_reshaped})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed_reshaped = pd.DataFrame(scaler.inverse_transform(data_imputed.reshape(-1,7)), columns=data_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed_reshaped['encounterId'] = data_id_intervalle['encounterId']\n",
    "data_imputed_reshaped['intervalle'] = data_id_intervalle['intervalle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed_reshaped = data_imputed_reshaped[data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed_reshaped['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed_reshaped[data_imputed_reshaped['intervalle'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imputed_reshaped.to_parquet(OUTPUT_TABLE + 'first_48_with_static_imputed_saits.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de la r√©p√©tabilit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Pr√©traitement des donn√©es\n",
    "unscaled_df = test_data.drop(['encounterId', 'intervalle'], axis=1).to_numpy()\n",
    "scaler = StandardScaler().fit(unscaled_df)\n",
    "\n",
    "test_data.to_parquet(OUTPUT_DATASET + 'original.parquet')\n",
    "\n",
    "# Sc√©narios √† tester\n",
    "filtered_conditions = [\n",
    "    ('Random', Test_mcar),\n",
    "    ('Single_row', Test_single_row),\n",
    "    ('Two_rows', Test_two_rows),\n",
    "    ('Three_rows', Test_three_rows),\n",
    "    ('pa_only', Test_pa)\n",
    "]\n",
    "\n",
    "datasets = {'lin_interpol': {}, 'saits': {}, 'mean': {}}\n",
    "idx_list = [(0, 'hr'), (1, 'spo2'), (2, 'fr'), (3, 'pam')]\n",
    "\n",
    "for scenario_name, scenario_data in filtered_conditions:\n",
    "    print(f'Processing scenario: {scenario_name}')\n",
    "    descaled_cond = scaler.inverse_transform(scenario_data.reshape(-1, n_features))\n",
    "\n",
    "    imputations = {\n",
    "        'saits': saits.impute({'X': scenario_data}),\n",
    "        'lin_interpol': lin_interpol(scenario_data),\n",
    "        'mean': impute_with_statistic(scenario_data)\n",
    "    }\n",
    "    print('Imputation done')\n",
    "\n",
    "    imputations_descaled = {\n",
    "        'saits' : scaler.inverse_transform(imputations['saits'].reshape(-1, n_features)),\n",
    "        'lin_interpol' : scaler.inverse_transform(imputations['lin_interpol'].reshape(-1, n_features)),\n",
    "        'mean' : scaler.inverse_transform(imputations['mean'].reshape(-1, n_features))\n",
    "    }\n",
    "\n",
    "    for method, df in imputations_descaled.items() :\n",
    "        df_parquet = pd.DataFrame(df, columns=['heart_rate', 'spo2', 'fr', 'pam',\n",
    "       'gender', 'age', 'admission_type'])\n",
    "        df_parquet['encounterId'] = test_data['encounterId']\n",
    "        df_parquet['intervalle'] = test_data['intervalle']\n",
    "        df_parquet.to_parquet(OUTPUT_DATASET + f'{scenario_name}_imputed_{method}.parquet')\n",
    "\n",
    "    for idx, feature_name in idx_list:\n",
    "        print(f'Processing feature: {feature_name}')\n",
    "        indicating_mask_test = np.isnan(descaled_cond[:, idx]) ^ np.isnan(unscaled_df[:, idx])\n",
    "        original_values = unscaled_df[:, idx][indicating_mask_test]\n",
    "\n",
    "        for method, imputed_data in imputations_descaled.items():\n",
    "            imputed_values = imputed_data[:, idx][indicating_mask_test]\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'masqu√©es': original_values,\n",
    "                'imput√©es': imputed_values\n",
    "            })\n",
    "        \n",
    "            df['moyenne'] = (df['masqu√©es'] + df['imput√©es']) / 2\n",
    "            df['diff√©rence'] = df['imput√©es'] - df['masqu√©es']\n",
    "\n",
    "                \n",
    "            output_dir = Path(OUTPUT_TABLE) / f'{feature_name}_comparaison'\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "\n",
    "            file_name = f'{scenario_name}_{method}.xlsx'\n",
    "            df.to_excel(output_dir / file_name, index=False)\n",
    "\n",
    "print('Processing complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pr√©traitement des donn√©es\n",
    "unscaled_df = test_data.drop(['encounterId', 'intervalle'], axis=1).to_numpy()\n",
    "scaler = StandardScaler().fit(unscaled_df)\n",
    "\n",
    "test_data.to_parquet(OUTPUT_DATASET + 'original.parquet')\n",
    "\n",
    "# Sc√©narios √† tester\n",
    "filtered_conditions = [\n",
    "    ('Random', Test_mcar),\n",
    "    ('Single_row', Test_single_row),\n",
    "    ('Two_rows', Test_two_rows),\n",
    "    ('Three_rows', Test_three_rows),\n",
    "    ('pa_only', Test_pa)\n",
    "]\n",
    "\n",
    "datasets = {'lin_interpol': {}, 'saits': {}, 'mean': {}}\n",
    "idx_list = [(0, 'hr'), (1, 'spo2'), (2, 'fr'), (4, 'pam')]\n",
    "\n",
    "# Progression des sc√©narios\n",
    "for scenario_name, scenario_data in tqdm(filtered_conditions, desc=\"Processing Scenarios\"):\n",
    "    tqdm.write(f'‚Üí Sc√©nario en cours : {scenario_name}')\n",
    "    descaled_cond = scaler.inverse_transform(scenario_data.reshape(-1, n_features))\n",
    "\n",
    "    imputations = {\n",
    "        'saits': saits.impute({'X': scenario_data}),\n",
    "        'lin_interpol': lin_interpol(scenario_data),\n",
    "        'mean': impute_with_statistic(scenario_data)\n",
    "    }\n",
    "    \n",
    "    tqdm.write(f'‚úî Imputation termin√©e pour {scenario_name}')\n",
    "\n",
    "    imputations_descaled = {\n",
    "        'saits' : scaler.inverse_transform(imputations['saits'].reshape(-1, n_features)),\n",
    "        'lin_interpol' : scaler.inverse_transform(imputations['lin_interpol'].reshape(-1, n_features)),\n",
    "        'mean' : scaler.inverse_transform(imputations['mean'].reshape(-1, n_features))\n",
    "    }\n",
    "\n",
    "    for method, df in imputations_descaled.items():\n",
    "        df_parquet = pd.DataFrame(df, columns=['heart_rate', 'spo2', 'fr', 'pam','gender', 'age', 'admission_type'])\n",
    "        df_parquet['encounterId'] = test_data['encounterId']\n",
    "        df_parquet['intervalle'] = test_data['intervalle']\n",
    "        df_parquet.to_parquet(OUTPUT_DATASET + f'{scenario_name}_imputed_{method}.parquet')\n",
    "\n",
    "    # Progression des features\n",
    "    for idx, feature_name in tqdm(idx_list, desc=f\"Processing Features for {scenario_name}\", leave=False):\n",
    "        indicating_mask_test = np.isnan(descaled_cond[:, idx]) ^ np.isnan(unscaled_df[:, idx])\n",
    "        original_values = unscaled_df[:, idx][indicating_mask_test]\n",
    "\n",
    "        for method, imputed_data in imputations_descaled.items():\n",
    "            imputed_values = imputed_data[:, idx][indicating_mask_test]\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'masqu√©es': original_values,\n",
    "                'imput√©es': imputed_values\n",
    "            })\n",
    "        \n",
    "            df['moyenne'] = (df['masqu√©es'] + df['imput√©es']) / 2\n",
    "            df['diff√©rence'] = df['imput√©es'] - df['masqu√©es']\n",
    "\n",
    "            output_dir = Path(OUTPUT_TABLE) / f'{feature_name}_comparaison'\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            file_name = f'{scenario_name}_{method}.xlsx'\n",
    "            df.to_excel(output_dir / file_name, index=False)\n",
    "\n",
    "tqdm.write('‚úî Traitement complet ! üöÄ')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
