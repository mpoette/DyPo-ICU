{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9432cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import json as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ffc925",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../params.json', 'r') as file :\n",
    "    params = json.load(file)\n",
    "\n",
    "DATASET, VERSION, DATA_FOLD = params['dataset'], params['version'], params['data_folder']\n",
    "\n",
    "print(f'Working on {DATASET} dataset {VERSION}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a468cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal = pd.read_parquet(f'{DATA_FOLD}/{VERSION}/2.clean_data/{DATASET}/temporal/treated_all_with_placeholder_values.parquet')\n",
    "static = pd.read_parquet(f'{DATA_FOLD}/{VERSION}/2.clean_data/{DATASET}/static/clean_static_encounters.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cfffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "static.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_subset_cols = ['encounterId', 'gender', 'age','poids_admission','taille']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_subset = static.loc[:,static_subset_cols]\n",
    "static_subset['imc'] = round(static_subset['poids_admission']/((static_subset['taille']/100)**2), 2)\n",
    "cond_1 = static_subset['imc'].between(5,100)\n",
    "static_subset['gender'] = static_subset['gender'] == 'Masculin'\n",
    "static_subset = static_subset.loc[cond_1,:].drop(columns=['taille', 'poids_admission'])\n",
    "static_subset['encounterId'] = static_subset['encounterId'].astype('int32')\n",
    "static_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63792e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_cols = ['los','deces_datediff','unitLabel' ]\n",
    "outcomes_subset = static[outcome_cols]\n",
    "#outcomes_subset['alive_j28'] = outc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e8852",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_cols = ['encounterId', 'delta_hour','fr', 'pam','heart_rate','spo2', 'temp','nad_dose_poids', 'is_ventilated', 'iv_input', 'fio2_corr', 'urine_rate']\n",
    "temporal_subset = temporal[temporal_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_null = pl.DataFrame(temporal[temporal_cols]).filter(pl.any_horizontal(temporal_cols).is_null())['encounterId'].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_subset = temporal_subset[~temporal_subset['encounterId'].isin(encounter_null)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57952717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Suppose que df est ton DataFrame d'origine\n",
    "# Et que 'var_cols' contient la liste de tes colonnes de variables (hors 'encounterId', 'delta_hour', etc.)\n",
    "var_cols = [col for col in temporal_subset.columns if col not in ['encounterId', 'delta_hour', 'last_timestamp']]\n",
    "\n",
    "# 1. Identifier le dernier timestamp valide pour chaque patient\n",
    "def get_max_valid_timestamp(group):\n",
    "    # On filtre les lignes qui ont au moins une variable non nulle\n",
    "    has_value = group[var_cols].notna().any(axis=1)\n",
    "    return group.loc[has_value, 'delta_hour'].max()\n",
    "\n",
    "# Calcul du max delta_hour à conserver pour chaque patient\n",
    "max_timestamps = temporal_subset.groupby('encounterId').apply(get_max_valid_timestamp).reset_index()\n",
    "max_timestamps.columns = ['encounterId', 'max_valid_delta_hour']\n",
    "\n",
    "# 2. Générer toutes les lignes nécessaires\n",
    "all_rows = []\n",
    "\n",
    "for _, row in max_timestamps.iterrows():\n",
    "    encounter_id = row['encounterId']\n",
    "    max_hour = int(row['max_valid_delta_hour'])\n",
    "    for h in range(0, max_hour + 1):\n",
    "        all_rows.append((encounter_id, h))\n",
    "\n",
    "full_index = pd.DataFrame(all_rows, columns=['encounterId', 'delta_hour'])\n",
    "\n",
    "# 3. Fusion avec le dataset original\n",
    "df_filled = pd.merge(full_index, temporal_subset, on=['encounterId', 'delta_hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac827fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0893fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_delta = df_filled[['encounterId', 'delta_hour']].groupby('encounterId')[['encounterId', 'delta_hour']].agg('max')\n",
    "encounter_sup_120 = max_delta[max_delta['delta_hour'] >= 120]['encounterId'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_5_days = df_filled[df_filled['delta_hour'].between(0,120) & df_filled['encounterId'].isin(encounter_sup_120)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f78cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7423b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_interpolate = ['fr', 'pam', 'heart_rate', 'spo2', 'temp', 'nad_dose_poids', 'fio2_corr']\n",
    "column_ffill_bfill = 'is_ventilated'\n",
    "\n",
    "# Étendre le dataset pour s'assurer que chaque encounterId ait toutes les heures de 0 à 120\n",
    "def expand_and_impute(group):\n",
    "    # Créer un index complet de 0 à 120\n",
    "    full_range = pd.DataFrame({'delta_hour': np.arange(0, 121)})\n",
    "    group = pd.merge(full_range, group, on='delta_hour', how='left')\n",
    "    group['encounterId'] = group['encounterId'].ffill().bfill()  # Réassigner l'id\n",
    "    # Imputer les valeurs numériques par interpolation\n",
    "    group[columns_to_interpolate] = group[columns_to_interpolate].interpolate(method='linear', limit_direction='both')\n",
    "    # Pour is_ventilated : ffill puis bfill\n",
    "    group[column_ffill_bfill] = group[column_ffill_bfill].ffill().bfill()\n",
    "    return group\n",
    "\n",
    "# Appliquer par groupe\n",
    "df_complete = temporal_5_days.groupby('encounterId', group_keys=False).apply(expand_and_impute)\n",
    "\n",
    "# (Optionnel) réordonner les colonnes\n",
    "cols = ['encounterId', 'delta_hour'] + columns_to_interpolate + [column_ffill_bfill]\n",
    "df_complete = df_complete[cols].merge(static_subset, on = 'encounterId', how='inner')\n",
    "df_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['imc'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = df_complete[['encounterId', 'delta_hour']].merge(static_subset[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f823a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "numpy_2d = df_complete[['fr', 'pam', 'heart_rate', 'spo2', 'temp','nad_dose_poids',  'age', 'imc','gender', 'is_ventilated']].astype('float').to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(numpy_2d[:, :8])  # Standardiser les colonnes numériques\n",
    "\n",
    "numpy_2d[:, :8] = scaler.transform(numpy_2d[:, :8])\n",
    "numpy_2d[:, 8:] = numpy_2d[:, 8:].astype(int)\n",
    "\n",
    "\n",
    "numpy_3d = numpy_2d.reshape(-1, 121, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.utils import to_time_series_dataset\n",
    "\n",
    "format_dataset = to_time_series_dataset(numpy_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a43cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = format_dataset[800:1200,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b359f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans, silhouette_score\n",
    "silhouettes = []\n",
    "for i in range(2, 6):  # start from 2 clusters\n",
    "    km = TimeSeriesKMeans(n_clusters=i, metric=\"dtw\")\n",
    "    labels = km.fit_predict(subset)\n",
    "    silhouette = silhouette_score(subset, labels, metric=\"dtw\")\n",
    "    print(silhouette)\n",
    "    silhouettes.append(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbee386",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utiliser 3 clusters sur le subset\n",
    "n_clusters = 3\n",
    "km = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", random_state=0)\n",
    "labels = km.fit_predict(subset)\n",
    "\n",
    "# Moyenne sur le temps pour chaque série pour réduire en 2D/3D\n",
    "X_flat = subset.mean(axis=1)\n",
    "\n",
    "# Réduction de dimension à 3 composantes principales\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_flat)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_pca[:,0], X_pca[:,1], X_pca[:,2], c=labels, cmap='viridis', alpha=0.7)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "plt.title('Visualisation 3D des clusters (k=3)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
