{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "QUERIES_FOLDER = \"../1.query/dynamic_features/\"\n",
    "OUTPUT_FOLDER = '../2.rawDataset/dynamic_features/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters = pl.read_parquet('../4.preparedDatasets/clean_static_encounters.parquet').select(\"encounterId\", \"utcInTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = 'localhost'\n",
    "DATABASE = 'mimiciv'\n",
    "conn_uri = f'postgresql://postgres:password@{HOST}/{DATABASE}'\n",
    "\n",
    "query = open(QUERIES_FOLDER + 'generic_query.sql').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../1.query/dynamic_features/' + 'features.json', 'r', encoding='utf-8') as f :\n",
    "    variables_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération des datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatisée via JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(variables_json.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_list = ['pam_non_invasive']\n",
    "variable_extraction = list(variables_json.keys()) # {key: variables_json[key] for key in variables_list} / list(variables_json.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in variable_extraction :\n",
    "    print(f'---------Extraction {variable} : START ---------')\n",
    "\n",
    "    loop_time = time.time()\n",
    "\n",
    "    # Récupération des paramètres pour l'extraction depuis le json\n",
    "    propname = variables_json[variable]['dictionary_propname']\n",
    "    table = variables_json[variable]['table']\n",
    "    feature = variables_json[variable]\n",
    "\n",
    "    # Implémentation des paramètres dans la requête SQL\n",
    "    query_formatted = query.format(\n",
    "        dictionaryPropName = propname,\n",
    "        feature = f\"'{variable}'\",\n",
    "        feature_table = table\n",
    "    )\n",
    "print(query_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_database_uri(query_formatted, conn_uri, engine='connectorx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for variable in variable_extraction :\n",
    "    print(f'---------Extraction {variable} : START ---------')\n",
    "\n",
    "    loop_time = time.time()\n",
    "\n",
    "    # Récupération des paramètres pour l'extraction depuis le json\n",
    "    propname = variables_json[variable]['dictionary_propname']\n",
    "    table = variables_json[variable]['table']\n",
    "    feature = variables_json[variable]\n",
    "\n",
    "    # Implémentation des paramètres dans la requête SQL\n",
    "    query_formatted = query.format(\n",
    "        dictionaryPropName = propname,\n",
    "        feature = f\"'{variable}'\",\n",
    "        feature_table = table\n",
    "    )\n",
    "    \n",
    "    try :\n",
    "\n",
    "        # Extraction des données et stockage dans un dataframe polars\n",
    "        feature_data = pl.read_database_uri(query_formatted, conn_uri, engine='connectorx')\n",
    "\n",
    "        print(f'Extraction {variable} : OK')\n",
    "\n",
    "        # Récupération uniquement des encounterId contenu dans le dataset 'statique'\n",
    "        with_time_delta = (feature_data\n",
    "        .rename(\n",
    "           { 'encounterid': 'encounterId',\n",
    "            'utccharttime': 'utcChartTime',\n",
    "            'valuestring': 'valueString',\n",
    "            'valuenumber': 'valueNumber'\n",
    "            }\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col('encounterId').cast(pl.String)\n",
    "        ).join(\n",
    "            encounters, on='encounterId', how=\"inner\"\n",
    "        ).with_columns(\n",
    "            (((pl.col('utcChartTime') - pl.col('utcInTime')).dt.total_minutes())/60).alias('delta_inTime_hours')\n",
    "        ).sort('encounterId', 'utcChartTime')\n",
    "        )\n",
    "        print(f\"Nombre d'encounters distincts : {with_time_delta.unique('encounterId').shape[0]}/{encounters.shape[0]}\")\n",
    "        try :\n",
    "            # Sauvegarde du dataset sous un format .parquet\n",
    "            with_time_delta.write_parquet(f'../2.rawDataset/dynamic_features/{variable}.parquet')\n",
    "            print(f'Sauvegarde {variable} : OK (shape : {with_time_delta.shape})')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(query)\n",
    "        continue\n",
    "    \n",
    "    end_loop_time = time.time()    \n",
    "    print(f'Elapsed time for {variable} : {round((end_loop_time - loop_time)/60, 0)}min (total exec : {round((end_loop_time-start)/60, 1)}min)')\n",
    "    print(f'---------Extraction {variable} : END ---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_encounters_folder = os.path.join(OUTPUT_FOLDER, 'missing_encounters')\n",
    "for filename in os.listdir(OUTPUT_FOLDER):\n",
    "    \n",
    "    if filename.endswith(\".parquet\"):\n",
    "        print(f'--------{filename}----------')\n",
    "        # Charger le dataset .parquet\n",
    "        feature_data = pl.read_parquet(os.path.join(OUTPUT_FOLDER, filename))\n",
    "        \n",
    "        # Récupérer les encounterId du dataset .parquet\n",
    "        feature_encounters = feature_data.select(\"encounterId\").unique()\n",
    "        \n",
    "        # Trouver les lignes du dataset encounters n'apparaissant pas dans la feature\n",
    "        missing_encounters = encounters.join(feature_encounters, on=\"encounterId\", how=\"anti\")\n",
    "        \n",
    "        # Afficher le résultat\n",
    "        print(f\"Encounters manquants: {missing_encounters.shape[0]}/{encounters.unique('encounterId').shape[0]}\")\n",
    "        missing_filename = os.path.join(missing_encounters_folder, f\"missing_{filename}\")\n",
    "        missing_encounters.write_parquet(missing_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
