{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré-traitement des fichiers \".parquet\" correspondant à des séries temporelles\n",
    "\n",
    "Dataset de sortie :  \n",
    "- une ligne par heure et par patient entre son entrée et sa sortie\n",
    "- une colonne par feature\n",
    "-  Nan si pas de valeur pour cette heure et ce patient\n",
    "- optionnel : 2e dataset censuré à 7 jours. Pour les patients sorties avant : Comblement avec Nan jusqu'à J7\n",
    "\n",
    "1. import des séries  \n",
    "2. Retrait des valeurs antérieurs à l'admission (offset de -1h en cas de bug à l'admission)  \n",
    "3. regroupement des données par heure \n",
    "    - pour les pressions artérielles : regroupement invasif et non invasif avec priorité sur l'invasif\n",
    "4. Ajout de NaN sur les heures manquantes (entre 1ère heure et dernière heure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../params.json', 'r') as file :\n",
    "    params = json.load(file)\n",
    "\n",
    "DATASET, VERSION = params['dataset'], params['version']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes\n",
    "CENSUS_FILE = f'/data2/poette.m/dypo/{VERSION}/2.clean_data/{DATASET}/static/clean_static_encounters.parquet'\n",
    "INPUT_FOLDER = f'/data2/poette.m/dypo/{VERSION}/1.raw_data/{DATASET}/dynamic_features/'\n",
    "OUTPUT_FOLDER = f'/data2/poette.m/dypo/{VERSION}/2.clean_data/{DATASET}/temporal/'\n",
    "with open('temporal_range.json', 'r') as f:\n",
    "    temporal_range = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Nettoyage du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pl.read_parquet(INPUT_FOLDER + 'pam_invasive.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleared_df(df, feature) :\n",
    "    feature_range = feature.replace('_non_invasive', '').replace('_invasive', '')\n",
    "    \n",
    "    print(feature)\n",
    "    if feature in ['pep', 'fio2'] : \n",
    "        lower_bound, upper_bound = temporal_range[feature_range]['range']\n",
    "    else : \n",
    "        # A ajouter Loop par patient \n",
    "        mean = df['valueNumber'].mean()\n",
    "        ds = df['valueNumber'].std()\n",
    "        stat_lower_bound = mean - (3 * ds)\n",
    "        stat_upper_bound = mean + 3*ds\n",
    "        set_lower_bound, set_upper_bound = temporal_range[feature_range]['range']\n",
    "        lower_bound = max(stat_lower_bound, set_lower_bound)\n",
    "        upper_bound = min(stat_upper_bound, set_upper_bound)\n",
    "    df_cleared = (df\n",
    "        .select(\n",
    "                ['encounterId', 'delta_inTime_hours', 'valueNumber', 'feature']\n",
    "        )\n",
    "        # Retrait des données antérieurs à l'admission et des valeurs hors range\n",
    "        .filter(\n",
    "            pl.col('delta_inTime_hours') >= -1,\n",
    "            pl.col('valueNumber').is_between(lower_bound, upper_bound)\n",
    "        )\n",
    "        # Troncature de l'intervalle de la données par rapport à l'entrée             \n",
    "        .with_columns(\n",
    "            (pl.col(\"delta_inTime_hours\").cast(pl.Int64)).alias('intervalle')\n",
    "        )\n",
    "        \n",
    "        .group_by(\n",
    "            'encounterId', 'intervalle'\n",
    "        )\n",
    "        .agg(\n",
    "            pl.col(\"valueNumber\").median().alias(feature)\n",
    "        )\n",
    "        .sort(\n",
    "            'encounterId', 'intervalle'\n",
    "        )\n",
    "    )\n",
    "    return df_cleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleared_df(df, feature):\n",
    "    feature_range = feature.replace('_non_invasive', '').replace('_invasive', '')\n",
    "    \n",
    "    lower_bound, upper_bound = temporal_range[feature_range]['range']\n",
    "    \n",
    "    # Calculer la moyenne et l'écart type pour chaque encounterId\n",
    "    stats_df = df.group_by('encounterId').agg([\n",
    "        pl.col('valueNumber').mean().alias('mean'),\n",
    "        pl.col('valueNumber').std().alias('std')\n",
    "    ])\n",
    "    \n",
    "    # Joindre les statistiques avec le dataframe original\n",
    "    df = df.join(stats_df, on='encounterId')\n",
    "    \n",
    "    # Calculer les bornes dynamiques pour chaque encounterId\n",
    "    df = df.with_columns([\n",
    "        (pl.col('mean') - 2 * pl.col('std')).alias('dynamic_lower_bound'),\n",
    "        (pl.col('mean') + 2 * pl.col('std')).alias('dynamic_upper_bound')\n",
    "    ])\n",
    "    \n",
    "    # Appliquer les filtres\n",
    "    df_cleared = (df\n",
    "        .select(\n",
    "            ['encounterId', 'delta_inTime_hours', 'valueNumber', 'feature', 'dynamic_lower_bound', 'dynamic_upper_bound']\n",
    "        )\n",
    "        # Retrait des données antérieures à l'admission et des valeurs hors range\n",
    "        .filter(\n",
    "            (pl.col('delta_inTime_hours') >= -1) &\n",
    "            (pl.col('valueNumber').is_between(lower_bound, upper_bound)) &\n",
    "            (pl.col('valueNumber').is_between(pl.col('dynamic_lower_bound'), pl.col('dynamic_upper_bound')))\n",
    "        )\n",
    "        # Troncature de l'intervalle de la donnée par rapport à l'entrée\n",
    "        .with_columns(\n",
    "            (pl.col(\"delta_inTime_hours\").cast(pl.Int64)).alias('intervalle')\n",
    "        )\n",
    "        .group_by(\n",
    "            'encounterId', 'intervalle'\n",
    "        )\n",
    "        .agg(\n",
    "            pl.col(\"valueNumber\").median().alias(feature)\n",
    "        )\n",
    "        .sort(\n",
    "            'encounterId', 'intervalle'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df_cleared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.3.2 Traitements des valeurs de pression artérielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressures_features = ['pam', 'pas', 'pad']\n",
    "non_invasive_pressures = [pressure +'_non_invasive' for pressure in pressures_features]\n",
    "invasive_pressures = [pressure +'_invasive' for pressure in pressures_features]\n",
    "\n",
    "# Fonction pour traiter les datasets de pressions\n",
    "def merged_pressure(invasive_df, non_invasive_df, feature_name):\n",
    "    # Lire les datasets invasifs et non invasifs\n",
    "\n",
    "    # Fusionner les datasets en priorisant les valeurs invasives\n",
    "    merged_df = (\n",
    "        invasive_df\n",
    "        .join(\n",
    "            non_invasive_df, on=[\"encounterId\", \"intervalle\"], how=\"full\"\n",
    "        )\n",
    "        .with_columns(\n",
    "            # Priorité aux valeurs invasives, compléter avec non-invasives si nécessaire\n",
    "            pl.coalesce([pl.col(f'{feature_name}_invasive'), pl.col(f'{feature_name}_non_invasive')]).alias(feature_name)\n",
    "        )\n",
    "        .with_columns(\n",
    "           pl.col(\"encounterId\").fill_null(pl.col(\"encounterId_right\")),\n",
    "           pl.col(\"intervalle\").fill_null(pl.col(\"intervalle_right\"))\n",
    "        )\n",
    "        .select(\n",
    "            ['encounterId', 'intervalle', feature_name]\n",
    "        )\n",
    "\n",
    "    )\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Comblement des intervalles manquants par Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_intervalle(df, feature) :\n",
    "    df_with_null = (\n",
    "        df\n",
    "        # Trouver l'intervalle maximum pour chaque patient\n",
    "        .group_by(\"encounterId\")\n",
    "        \n",
    "        .agg([\n",
    "            pl.col(\"intervalle\").max().alias(\"max_inter\")\n",
    "        ])\n",
    "\n",
    "        # Étendre l'intervalle pour inclure toutes les heures entre 0 et max_hour\n",
    "        .with_columns([\n",
    "            pl.struct(\n",
    "                [\"encounterId\", \"max_inter\"]\n",
    "                ).map_elements(\n",
    "                lambda row: list(range(0, row[\"max_inter\"] + 1)),\n",
    "                return_dtype=pl.List(pl.Int64)\n",
    "                ).alias(\"all_hours\")\n",
    "        ])\n",
    "\n",
    "        # Exploser les heures dans une nouvelle ligne\n",
    "        .explode(\"all_hours\")\n",
    "\n",
    "        # Joindre avec les données existantes pour aligner les heures\n",
    "        .join(\n",
    "            df,\n",
    "            left_on=[\"encounterId\", \"all_hours\"],\n",
    "            right_on=[\"encounterId\", \"intervalle\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Remplacer les valeurs manquantes par des NaN dans les colonnes de features\n",
    "\n",
    "        .with_columns([\n",
    "            pl.col(feature).fill_null(float('nan')) for feature in df.columns if feature not in [\"encounterId\", \"intervalle\"]\n",
    "        ])\n",
    "        \n",
    "        # Renommer les colonnes pour uniformité\n",
    "        .rename({\"all_hours\": \"intervalle\"})\n",
    "\n",
    "        # Réorganiser les colonnes\n",
    "        .select([\"encounterId\", \"intervalle\", feature])\n",
    "    )\n",
    "    return df_with_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters = pl.read_parquet(CENSUS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(INPUT_FOLDER):\n",
    "\n",
    "    if filename.endswith('.parquet'):\n",
    "\n",
    "\n",
    "        feature = os.path.splitext(filename)[0]\n",
    "\n",
    "        if feature not in (non_invasive_pressures + invasive_pressures) :\n",
    "            \n",
    "            raw_df = pl.read_parquet(os.path.join(INPUT_FOLDER, filename))\n",
    "            cleared = cleared_df(raw_df, feature)\n",
    "            with_missing_values = fill_missing_intervalle(cleared, feature)\n",
    "            \n",
    "            with_missing_values.write_parquet(os.path.join(OUTPUT_FOLDER, f'cleared_{filename}'))\n",
    "            print(f'ok {feature}')\n",
    "            \n",
    "        \n",
    "        elif feature in invasive_pressures :\n",
    "            pressure_feature = feature.replace('_invasive', '')\n",
    "            non_invasive_feature = f'{pressure_feature}_non_invasive'\n",
    "            raw_invasive = pl.read_parquet(os.path.join(INPUT_FOLDER, filename))\n",
    "            \n",
    "            raw_non_invasive = pl.read_parquet(os.path.join(INPUT_FOLDER, filename.replace('invasive', 'non_invasive')))\n",
    "\n",
    "            cleared_invasive = cleared_df(raw_invasive, feature)\n",
    "            cleared_non_invasive = cleared_df(raw_non_invasive, non_invasive_feature)\n",
    "            merged_df = merged_pressure(cleared_invasive, cleared_non_invasive, pressure_feature)\n",
    "            with_missing_values = fill_missing_intervalle(merged_df, pressure_feature)\n",
    "\n",
    "            with_missing_values.write_parquet(os.path.join(OUTPUT_FOLDER, f'cleared_{pressure_feature}.parquet'))\n",
    "            print(f'ok {pressure_feature}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge in 1 week dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_first_value(df) :\n",
    "    cleaned_dfs = []\n",
    "    features_col = list(set(df.columns) - set(['encounterId', 'intervalle']))\n",
    "    for encounterId, group in df.group_by(\"encounterId\"):\n",
    "        # Vérifie si au moins une variable n'est pas nulle\n",
    "        group = (group\n",
    "                    .fill_nan(None)\n",
    "                    .with_columns(\n",
    "                        pl.any_horizontal(features_col).is_not_null()\n",
    "                        .alias('has_data')\n",
    "                        )\n",
    "                )\n",
    "        # Trouver l'index de la première ligne où la variable has_data est True\n",
    "        first_valid_index = group.select(pl.col(\"has_data\")).to_pandas()[\"has_data\"].idxmax()\n",
    "\n",
    "        # Si aucun élément n'est valide, ignorer le patient\n",
    "        if first_valid_index == -1:\n",
    "            continue\n",
    "\n",
    "        # Garder les lignes à partir de la première valide\n",
    "        group = group[first_valid_index:]\n",
    "\n",
    "        # Réinitialiser l'intervalle pour commencer à zéro\n",
    "        group = group.with_columns(\n",
    "            (pl.col(\"intervalle\") - pl.lit(group[\"intervalle\"][0])).alias(\"intervalle\")\n",
    "        )\n",
    "        \n",
    "        # Supprimer la colonne temporaire \"has_data\"\n",
    "        group = group.drop(\"has_data\")\n",
    "\n",
    "\n",
    "        cleaned_dfs.append(group)\n",
    "\n",
    "    # Fusionner tous les groupes nettoyés\n",
    "    cleaned_df = pl.concat(cleaned_dfs)\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parquet_files(input_folder, static_df):\n",
    "    # Liste des `encounterId` uniques du dataset statique\n",
    "    encounter_ids = static_df[\"encounterId\"].unique()\n",
    "\n",
    "    # Création d'un dataframe de 0 à 180h \n",
    "    # 180h permet de prendre une marge de 12h supplémentaire afin de ne pas perdre de valeurs après suppression de l'offset\n",
    "    intervalle_series = pl.Series(\"intervalle\", range(0, 180))\n",
    "    intervalle_df = pl.DataFrame({\"intervalle\": intervalle_series})  \n",
    "      \n",
    "    # Créer l'intervalle standardisé de 0 à 180 heures pour chaque patient\n",
    "    standard_intervals = (\n",
    "        pl.DataFrame({\"encounterId\": encounter_ids})\n",
    "        .join(\n",
    "            intervalle_df,\n",
    "            how=\"cross\"  # Produit cartésien\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Contrôle du nombre de lignes du dataset des encounters\n",
    "    print(\"Standard intervals shape:\", standard_intervals.shape)\n",
    "\n",
    "    # Initialiser un dataframe standardisé\n",
    "    merged_df = standard_intervals\n",
    "\n",
    "    # Parcourir chaque fichier .parquet et le fusionner avec l'intervalle standardisé\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.endswith(\".parquet\"):\n",
    "            # Nom de la feature basée sur le nom du fichier\n",
    "            feature_name = os.path.splitext(file)[0]\n",
    "\n",
    "            # Charger le fichier .parquet\n",
    "            df = pl.read_parquet(os.path.join(input_folder, file))\n",
    "\n",
    "            # Joindre avec l'intervalle standardisé\n",
    "            merged_df = (\n",
    "                merged_df.join(\n",
    "                    df,\n",
    "                    on=[\"encounterId\", \"intervalle\"],\n",
    "                    how=\"left\"\n",
    "                )\n",
    "                .rename({feature_name: feature_name})  # Renommer pour conserver le nom original\n",
    "            )\n",
    "    merged_df = merged_df.drop(['pep', 'fio2'])\n",
    "    reset_intervalle_df = reset_first_value(merged_df)\n",
    "    \n",
    "    return reset_intervalle_df.filter(pl.col('intervalle') < 168)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected temporal dataset lenght : {encounters.unique('encounterId').shape[0]*168}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temporal_week = process_parquet_files(OUTPUT_FOLDER, encounters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6813180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_fold = f'/data2/poette.m/dypo/{VERSION}/3.analysis/times_series/{DATASET}/'\n",
    "temporal_week.write_parquet(temporal_fold + 'one_week.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_encounters_folder = os.path.join(INPUT_FOLDER, 'missing_encounters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for filename in os.listdir(OUTPUT_FOLDER):\n",
    "    print(f'--------{filename}----------')\n",
    "    if filename.endswith(\".parquet\"):\n",
    "        # Charger le dataset .parquet\n",
    "        feature_data = pl.read_parquet(os.path.join(OUTPUT_FOLDER, filename))\n",
    "        \n",
    "        # Récupérer les encounterId du dataset .parquet\n",
    "        feature_encounters = feature_data.select(\"encounterId\").unique()\n",
    "        \n",
    "        # Trouver les lignes du dataset encounters n'apparaissant pas dans la feature\n",
    "        missing_encounters = encounters.join(feature_encounters, on=\"encounterId\", how=\"anti\")\n",
    "        \n",
    "        # Afficher le résultat\n",
    "        print(f\"Encounters manquants: {missing_encounters.shape[0]}/{encounters.unique('encounterId').shape[0]}\")\n",
    "        missing_filename = os.path.join(missing_encounters_folder, filename.replace('cleared', 'missing'))\n",
    "        #missing_encounters.write_parquet(missing_filename)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
