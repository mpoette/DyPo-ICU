{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import lightning as L\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import json\n",
    "import polars as pl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../params.json', 'r') as file :\n",
    "    params = json.load(file)\n",
    "\n",
    "DATASET, VERSION, DATA_FOLD = params['dataset'], params['version'], params['data_folder']\n",
    "\n",
    "print(f'Working on {DATASET} dataset {VERSION}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(f'{DATA_FOLD}/{VERSION}/2.clean_data/{DATASET}/temporal/five_days_dataset.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr√©-traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_treated = (df\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('deces_datediff') < 5).then(pl.lit(5)).otherwise('deces_datediff')\n",
    "        ).with_columns(\n",
    "            pl.when((pl.col('deces_datediff')-pl.col('delta_hour')/24) < 1).then(True)\n",
    "            .otherwise(False)\n",
    "                .alias('survival_inf24'),\n",
    "            ((pl.col('deces_datediff') >= 90) | (pl.col('deces_datediff').is_null())).alias('j90_survival')\n",
    "        )\n",
    ")\n",
    "\n",
    "df_pre_treated['j90_survival'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_pre_treated.columns)\n",
    "df_pre_treated.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pam', 'pas', 'pad', 'heart_rate', 'spo2', 'nad_dose_poids', 'fio2_corr', 'is_ventilated', 'gender', 'age']\n",
    "targets = ['survival_inf24', 'j90_survival', 'los']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_set = df_pre_treated.select('encounterId', 'j90_survival').unique().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train et Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encounters, test_encounters = train_test_split(encounter_set, stratify = encounter_set['j90_survival'], test_size=0.3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "data_trans = df_pre_treated.to_pandas().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "le = LabelEncoder()\n",
    "\n",
    "numeric_data = ['pam', 'pas', 'pad', 'heart_rate', 'spo2', 'nad_dose_poids', 'fio2_corr','age']\n",
    "cat_data = ['is_ventilated','gender', 'survival_inf24', 'j90_survival']\n",
    "for n in numeric_data :\n",
    "    data_trans[n] = scaler.fit_transform(data_trans[[n]])\n",
    "\n",
    "for c in cat_data :\n",
    "    data_trans[c] = le.fit_transform(data_trans[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_trans.delta_hour.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =data_trans[['encounterId', 'delta_hour'] + features]\n",
    "Y = data_trans[['encounterId', 'delta_hour'] + targets]\n",
    "\n",
    "\n",
    "X_train = X[X['encounterId'].isin(train_encounters['encounterId'])].sort_values(by=['encounterId', 'delta_hour']).reset_index(drop=True)    \n",
    "y_train = Y[Y['encounterId'].isin(train_encounters['encounterId'])].sort_values(by=['encounterId', 'delta_hour']).reset_index(drop=True)  \n",
    "X_test = X[X['encounterId'].isin(test_encounters['encounterId'])].sort_values(by=['encounterId', 'delta_hour']).drop_duplicates(subset=['encounterId', 'delta_hour'], keep='first').reset_index(drop=True)  \n",
    "y_test = Y[Y['encounterId'].isin(test_encounters['encounterId'])].sort_values(by=['encounterId', 'delta_hour']).drop_duplicates(subset=['encounterId', 'delta_hour'], keep='first').reset_index(drop=True)  \n",
    "\n",
    "\n",
    "y_train_dropped = y_train.drop_duplicates('encounterId').sort_values(by='encounterId').reset_index(drop=True)\n",
    "y_test_dropped = y_test.drop_duplicates('encounterId').sort_values(by='encounterId').reset_index(drop=True)\n",
    "y_train_numpy = y_train_dropped['j90_survival'].to_numpy()\n",
    "y_test_numpy = y_test_dropped['j90_survival'].to_numpy()\n",
    "\n",
    "X_train_3d = X_train.drop(columns=['encounterId', 'delta_hour']).to_numpy().reshape(len(train_encounters), len(X_test.delta_hour.unique()), len(features))\n",
    "X_test_3d = X_test.drop(columns=['encounterId', 'delta_hour']).to_numpy().reshape(len(test_encounters), len(X_test.delta_hour.unique()), (len(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Early/Late/Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X_train_3d\n",
    "X_train_early = X_train_3d[:, :48, :]\n",
    "X_train_late = X_train_3d[:, -48:, :]\n",
    "\n",
    "\n",
    "X_test_full = X_test_3d\n",
    "X_test_early = X_test_3d[:, :48, :] \n",
    "X_test_late = X_test_3d[:, -48:, :]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_3d.shape)\n",
    "print(X_test_3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(L.LightningModule):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size=32):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.permute(1, 0, 2)\n",
    "        lstm_out, _ = self.lstm(input)\n",
    "        return self.fc(lstm_out[-1]).squeeze(-1)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.sigmoid(logits) > 0.5\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"full\":   (X_train_3d, y_train_numpy),\n",
    "    \"early\":  (X_train_3d[:, :48, :], y_train_numpy),\n",
    "    \"late\":   (X_train_3d[:, -48:, :], y_train_numpy)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "for config_name, (X, y) in configs.items():\n",
    "    print(f\"\\nüü¢ Training config: {config_name} | shape = {X.shape}\")\n",
    "\n",
    "    # Cr√©e le Dataset et DataLoader\n",
    "    inputs = torch.tensor(X).float()\n",
    "    labels = torch.tensor(y).float()\n",
    "    dataset = TensorDataset(inputs, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Instancie un mod√®le LSTM propre\n",
    "    model = LSTM(input_size=X.shape[2])  # n_features\n",
    "\n",
    "    # Trainer Lightning\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=100,\n",
    "        log_every_n_steps=2,\n",
    "        enable_progress_bar=True,\n",
    "        logger=False,  # tu peux activer un logger si besoin\n",
    "        enable_checkpointing=False\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloaders=dataloader)\n",
    "\n",
    "    trained_models[config_name] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = torch.tensor(X_test_3d).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, f1_score, roc_curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Suppose : trained_models[\"full\"], [\"early\"], [\"late\"]\n",
    "#           X_test_full, X_test_early, X_test_late\n",
    "#           y_test_numpy\n",
    "\n",
    "X_test_dict = {\n",
    "    \"full\": X_test_full,\n",
    "    \"early\": X_test_early,\n",
    "    \"late\": X_test_late\n",
    "}\n",
    "\n",
    "results = []\n",
    "roc_curves = {}\n",
    "calib_curves = {}\n",
    "\n",
    "for config_name, model in trained_models.items():\n",
    "    print(f\"‚è≥ √âvaluation : {config_name}\")\n",
    "    X = X_test_dict[config_name]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.tensor(X).float())\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "    y_pred_bin = (probs > 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_test_numpy, probs)\n",
    "    f1 = f1_score(y_test_numpy, y_pred_bin)\n",
    "\n",
    "    # Sauvegarde\n",
    "    results.append({\"Configuration\": config_name, \"AUC\": auc, \"F1 Score\": f1})\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test_numpy, probs)\n",
    "    roc_curves[config_name] = (fpr, tpr, auc)\n",
    "\n",
    "    prob_true, prob_pred = calibration_curve(y_test_numpy, probs, n_bins=10, strategy='quantile')\n",
    "    calib_curves[config_name] = (prob_pred, prob_true)\n",
    "\n",
    "# ‚ûï DataFrame r√©sultats\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nüìä Tableau r√©capitulatif :\")\n",
    "print(results_df)\n",
    "\n",
    "# üìà Courbe ROC\n",
    "plt.figure(figsize=(6, 5))\n",
    "for name, (fpr, tpr, auc) in roc_curves.items():\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# üìà Courbe de calibration\n",
    "plt.figure(figsize=(6, 5))\n",
    "for name, (pred, true) in calib_curves.items():\n",
    "    plt.plot(pred, true, marker='o', label=name)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"Proba pr√©dite (moyenne)\")\n",
    "plt.ylabel(\"Fr√©quence observ√©e\")\n",
    "plt.title(\"Calibration Curve (10 bins)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M√©triques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "auc = roc_auc_score(y_test_numpy, probs)\n",
    "print(f\"AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test_numpy, probs)\n",
    "\n",
    "f2_scores = 5 * (precision * recall) / (4 * precision + recall)\n",
    "best_idx = np.argmax(f2_scores)\n",
    "best_thresh = thresholds[best_idx]\n",
    "\n",
    "print(f\"Best F2-score: {f2_scores[best_idx]:.3f} at threshold {best_thresh:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_numpy, probs)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# y_test_numpy : vraies √©tiquettes binaires (0/1)\n",
    "# probs         : probabilit√©s pr√©dites (sigmoid√©es)\n",
    "\n",
    "# Calcule les probabilit√©s moyennes dans chaque bin\n",
    "prob_true, prob_pred = calibration_curve(y_test_numpy, probs, n_bins=10, strategy='quantile')\n",
    "\n",
    "# Affichage\n",
    "plt.figure()\n",
    "plt.plot(prob_pred, prob_true, marker='o', label=\"Mod√®le\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Id√©al\")\n",
    "plt.xlabel(\"Probabilit√© pr√©dite (moyenne par bin)\")\n",
    "plt.ylabel(\"Fr√©quence observ√©e\")\n",
    "plt.title(\"Courbe de calibration (10 bins)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
