{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nettoyage et préparation des données statiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des paramètres du projet\n",
    "with open('../params.json', 'r') as file :\n",
    "    params = json.load(file)\n",
    "\n",
    "DATASET, VERSION = params['dataset'], params['version'] # Nom du dataset et de la version\n",
    "DATA_FOLD = params['data_folder'] # Localisation des données (entrée et sortie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Localisation des fichiers\n",
    "INPUT_FOLDER = f'{DATA_FOLD}/{VERSION}/1.raw_data/{DATASET}/'\n",
    "OUTPUT_FOLDER = f'{DATA_FOLD}/{VERSION}/2.clean_data/{DATASET}/static/'\n",
    "\n",
    "CENSUS_FILE = 'census/raw_census.xlsx'\n",
    "IGS_FILE = 'igs/igs_all.csv'\n",
    "OUTPUT_STATIC_FILE = 'clean_static_encounters.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_percentage(value):\n",
    "    # Probabilité de décès par IGS II en valeur décimale\n",
    "    try:\n",
    "        clean_value = float(value.replace('Mortalité prédite : ', '').replace('%', '').replace(',', '.')) / 100\n",
    "        return clean_value\n",
    "    except :\n",
    "        return None\n",
    "\n",
    "def clean_encounter(value):\n",
    "    # Nettoyage des données sur la table des identifiants\n",
    "    try:\n",
    "        remove_letters = re.sub('\\D', '', value)\n",
    "        return remove_letters\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def clean_igs(value):\n",
    "    try:\n",
    "        clean_value = float(value)\n",
    "        return clean_value\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pl.read_excel(INPUT_FOLDER + CENSUS_FILE, schema_overrides={\"lifeTimeNumber\": pl.Int64}) # Données statiques brutes\n",
    "df_igs = pl.read_csv(INPUT_FOLDER + IGS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'mimic' :\n",
    "    raw_data = raw_data.rename(\n",
    "        # Renommage des variables Mimic pour correspondre aux noms sur le dataset CHU\n",
    "                    {\n",
    "                        'displaylabel': 'displayLabel',\n",
    "                        'encounterid': 'encounterId',\n",
    "                        'encounternumber': 'encounterNumber',\n",
    "                        'lifetimenumber': 'lifeTimeNumber',\n",
    "                        'dateofdeath' : 'dateOfDeath',\n",
    "                        'isdeceased' : 'isDeceased',\n",
    "                        'utcintime' : 'utcInTime',\n",
    "                        'utcouttime' : 'utcOutTime',\n",
    "                        'lengthofstay' : 'lengthOfStay',\n",
    "                        'height' : 'taille',\n",
    "                        'weight' : 'poids_admission'\n",
    "                    }\n",
    "                ).with_columns(\n",
    "                    # Création de colonnes vides pour les données nominatives de Mimic\n",
    "                    [\n",
    "                        pl.lit(None).alias('adresse'),\n",
    "                        pl.lit(None).alias('ville'),\n",
    "                        pl.lit(None).alias('cp'),\n",
    "                        pl.lit(None).alias('dateOfBirth'),\n",
    "                        pl.lit(None).alias('lastName'),\n",
    "                        pl.lit(None).alias('firstName')\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "if DATASET == 'chu' :\n",
    "    raw_data = raw_data.with_columns(\n",
    "        # Création de colonnes vides pour les données de taille / poids des données CHU\n",
    "                    [\n",
    "                        pl.lit(None).alias('taille'),\n",
    "                        pl.lit(None).alias('poids_admission'),\n",
    "                    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des unités de soins\n",
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate mortality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insee = pl.read_csv(f'{DATA_FOLD}/{VERSION}/1.raw_data/chu/census/deces_insee.csv', ignore_errors=True)\n",
    "df_mortality= (df_insee.filter(pl.col(\"upper\") == 1)\n",
    "    .cast(\n",
    "        {\"date_deces\" : pl.Date},\n",
    "        strict=False\n",
    "    )\n",
    "    .with_columns(\n",
    "            pl.when(pl.col('DECES_PASTEL') == \"NA\")\n",
    "            .then(pl.lit(None))\n",
    "            .otherwise(pl.lit(True))\n",
    "            .alias(\"deces_hosp\")\n",
    ")\n",
    "    .sort([\"NIP\", \"mean_score\", \"date_deces\"], descending=[True, True, False])\n",
    "    .unique(\"NIP\", keep=\"first\")\n",
    "    .drop(\n",
    "        ['id', 'probas_rf', 'probas_nn', 'upper','numero_acte_deces', 'mean_score', 'DECES_PASTEL' ]\n",
    "        )\n",
    "    .rename(\n",
    "        {\"NIP\" : \"lifeTimeNumber\"}\n",
    "    )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_mortality = raw_data.select('encounterId', 'dateOfDeath','isDeceased'\n",
    "                ).filter((pl.col('isDeceased') == True) | (pl.col('dateOfDeath').is_not_null())\n",
    "                         ).group_by('encounterId').agg(\n",
    "                             pl.col('isDeceased').max().alias('isDeceased'),\n",
    "                                pl.col('dateOfDeath').first().alias('dateOfDeath')\n",
    "                ).cast(\n",
    "                    {'encounterId' : pl.String}\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mortality.filter(pl.col(\"deces_hosp\") == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regroup by Encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrait des unités de chirurgie cardiaque dans les 2 datasets \n",
    "# Motif : Chirurgie majoritairement programmée, durée de séjours courte, mortalité faible et volume important de patients\n",
    "\n",
    "# Retrait des unités de réanimation des grands brûlés, néphrologique et pédiatrique dans le dataset CHU\n",
    "# Motif : Typologie de réanimation non présentes dans le dataset MIMIC\n",
    "if DATASET == 'chu' :\n",
    "    icu_units = [ \n",
    "                'PURPAN REA. POLY.',\n",
    "                'IUC REA.', \n",
    "                'NEURO-CHIR REA', \n",
    "                'RANGUEIL REA. POLY.',\n",
    "                'RANGUEIL DECHO. REA.', \n",
    "                'PURPAN DECHO. REA.', \n",
    "                'PURPAN SC. REA.', \n",
    "                'RANGUEIL SC. REA.',\n",
    "                'IUC SC.',\n",
    "                'C.C.V. REA',\n",
    "                'GRANDS BRULES',\n",
    "                'NEURO-CHIR USC'\n",
    "                ]\n",
    "    \n",
    "elif DATASET == 'mimic' :\n",
    "    icu_units = [\n",
    "                'Neuro Surgical Intensive Care Unit (Neuro SICU)',\n",
    "                'Surgical Intensive Care Unit (SICU)',\n",
    "                'Neuro Stepdown',\n",
    "                'Medical/Surgical Intensive Care Unit (MICU/SICU)',\n",
    "                'Cardiac Vascular Intensive Care Unit (CVICU)', #retrait chirurgie CCV pour correspondre au dataset CHU\n",
    "                'Neuro Intermediate',\n",
    "                'Coronary Care Unit (CCU)',\n",
    "                'Medical Intensive Care Unit (MICU)',\n",
    "                'Trauma SICU (TSICU)'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_ipp = [ \n",
    "                'PURPAN REA. POLY.',\n",
    "                'IUC REA.', \n",
    "                'NEURO-CHIR REA', \n",
    "                'RANGUEIL REA. POLY.',\n",
    "                'RANGUEIL DECHO. REA.', \n",
    "                'PURPAN DECHO. REA.', \n",
    "                'PURPAN SC. REA.', \n",
    "                'RANGUEIL SC. REA.',\n",
    "                'IUC SC.',\n",
    "                'C.C.V. REA',\n",
    "                'GRANDS BRULES',\n",
    "                'NEURO-CHIR USC'\n",
    "                \n",
    "]\n",
    "\n",
    "ipp_list = (raw_data\n",
    "                .filter(\n",
    "                    # Suppression des unités non désirées\n",
    "                    pl.col('displayLabel').is_in(units_ipp),\n",
    "                    # Suppression des patients mineurs lors de leur admission\n",
    "                    (pl.col('age') >= 18)\n",
    "                    )\n",
    "                .with_columns(\n",
    "                    # Nettoyrage des encounterId\n",
    "                    pl.col('encounterId').cast(pl.String).map_elements(clean_encounter, return_dtype=pl.String).alias('encounterId')\n",
    "                    )\n",
    "                .sort(by=['encounterId', 'utcInTime'])\n",
    ")\n",
    "\n",
    "ipp_list.unique('lifeTimeNumber').select('lifeTimeNumber', 'utcInTime')#.write_csv('/data2/poette.m/dypo/v4/1.raw_data/chu/census/ipp_pour_matching.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_df = (raw_data\n",
    "                .filter(\n",
    "                    # Suppression des unités non désirées\n",
    "                    pl.col('displayLabel').is_in(icu_units),\n",
    "                    # Suppression des patients mineurs lors de leur admission\n",
    "                    (pl.col('age') >= 18)\n",
    "                    )\n",
    "                .with_columns(\n",
    "                    # Nettoyrage des encounterId\n",
    "                    pl.col('encounterId').cast(pl.String).map_elements(clean_encounter, return_dtype=pl.String).alias('encounterId')\n",
    "                    )\n",
    "                .sort(by=['encounterId', 'utcInTime'])\n",
    "                .group_by(\n",
    "                    ['encounterId',\n",
    "                    'encounterNumber',\n",
    "                    'lifeTimeNumber',\n",
    "                    'lastName',\n",
    "                    'firstName',\n",
    "                    'gender',\n",
    "                    'age',\n",
    "                    'dateOfBirth',\n",
    "                    'lengthOfStay']\n",
    "                )\n",
    "                .agg([\n",
    "                # Récupération des dates d'entrée minimales et de sortie maximale pour chaque séjour\n",
    "                # Séléction de la première unité de réanimation \n",
    "                    pl.col('utcInTime').min().alias('utcInTime'),\n",
    "                    pl.col('utcOutTime').max().alias('utcOutTime'),\n",
    "                    pl.col(\"displayLabel\")\n",
    "                    .filter((pl.col(\"displayLabel\").is_not_null()))\n",
    "                    .sort_by('lengthOfStay', descending=True)\n",
    "                    .first()\n",
    "                    .alias('unitLabel'),\n",
    "                    pl.col('adresse').max().alias('adresse'),\n",
    "                    pl.col('ville').max().alias('ville'),\n",
    "                    pl.col('cp').max().alias('cp'),\n",
    "                    pl.col('taille').first().alias('taille'),\n",
    "                    pl.col('poids_admission').first().alias('poids_admission')\n",
    "                ])\n",
    "                .join(\n",
    "                    # Jointure sur le dataset de mortalité\n",
    "                    df_mortality, on='lifeTimeNumber', how='left'\n",
    "                ).with_columns(\n",
    "                    [\n",
    "                    # Durée de séjour en heures (date de sortie max - date d'entrée min)\n",
    "                        ((((pl.col('utcOutTime').sub(pl.col('utcInTime'))).dt.total_minutes())/60)).round(2).alias('los')\n",
    "                    ]\n",
    "                ).with_columns(\n",
    "                    # Survenue d'un décès défini comme la présence d'une date de décès ou du statut'décès' concernant le mode de sortie\n",
    "                    # Si ni l'un ni l'autre = survie\n",
    "                    pl.col('date_deces').is_not_null().alias(\"isDeceased\"),\n",
    "                    pl.col('deces_hosp').is_not_null().alias(\"deces_hosp\"),\n",
    "                    (pl.col('date_deces')- pl.col('utcInTime')).dt.total_days().alias('deces_datediff')\n",
    "                ).drop('lengthOfStay')\n",
    "\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_df.filter(pl.col('deces_datediff') > 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended demography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour le Dataset CHU : ajout des motifs d'entrée et de sortie (en texte libre) et ajout du poids et de la taille\n",
    "df_demo_extended = encounter_df\n",
    "if DATASET == 'chu': \n",
    "    df_demo_extended = df_demo_extended.drop(['taille', 'poids_admission'])\n",
    "    directory = INPUT_FOLDER + 'extended_demography/'\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".parquet\"): \n",
    "            df_extended_feature = pl.read_parquet(directory + filename)\n",
    "            print(df_extended_feature.shape)\n",
    "            feature = df_extended_feature.get_column('feature').to_list()[0]\n",
    "            print(feature)\n",
    "            df_extended_feature = df_extended_feature.sort(\n",
    "                            'encounterId', 'utcChartTime'\n",
    "                        ).unique(\n",
    "                            subset=['encounterId'], keep='last'\n",
    "                        )\n",
    "            if feature in ['taille', 'poids_admission'] :\n",
    "                df_extended_feature = df_extended_feature.rename(\n",
    "                        {'valueNumber' : feature}\n",
    "                )\n",
    "            else:\n",
    "                df_extended_feature = df_extended_feature.rename(\n",
    "                        {'valueString' : feature}\n",
    "                )\n",
    "            df_extended_feature = df_extended_feature.select('encounterId', feature)\n",
    "            df_demo_extended = df_demo_extended.join(\n",
    "                            df_extended_feature, on='encounterId', how='left'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_extended['unitLabel'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'chu': \n",
    "    ttt = pl.read_csv(INPUT_FOLDER + '/extended_demography/ttt.csv', separator=';').sort(by=['encounterId', 'storeTime']).unique('encounterId', keep='last')\n",
    "    atcd = pl.read_csv(INPUT_FOLDER + '/extended_demography/atcd.csv', separator=';').sort(by=['encounterId', 'storeTime']).unique('encounterId', keep='last')\n",
    "    ttt = ttt.select('encounterId', 'valueString').rename({'valueString' : 'traitements'}).cast({'encounterId' : pl.String})\n",
    "    atcd = atcd.select('encounterId', 'valueString').rename({'valueString' : 'atcd'}).cast({'encounterId' : pl.String})\n",
    "    df_demo_extended = df_demo_extended.join(ttt, on = 'encounterId', how='left').join(atcd, on = 'encounterId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_extended.join(ttt, on = 'encounterId', how='left').join(atcd, on = 'encounterId', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Mimic : ajout des données présentes dans l'IGS (mtype d'admission, score SAPS, probabilité de survie)\n",
    "if DATASET == 'mimic' :\n",
    "    df_igs_clean = (\n",
    "            df_igs  \n",
    "            .rename(\n",
    "                {\n",
    "                    'encounterid': 'encounterId'\n",
    "                    ,'admissiontype_score' : 'admission_type'\n",
    "                }\n",
    "            )\n",
    "            .sort('encounterId', 'sapsii', descending=[False, True])\n",
    "            .unique('encounterId', keep='first')\n",
    "            .with_columns(\n",
    "                pl.when(pl.col('admission_type') == 8)\n",
    "                    .then(pl.lit('Unscheduled Surgery'))\n",
    "                .when(pl.col('admission_type') == 0)\n",
    "                    .then(pl.lit('Scheduled Surgery'))\n",
    "                .when(pl.col('admission_type') == 6)\n",
    "                    .then(pl.lit('Medical'))\n",
    "                .otherwise(None).alias('admission_type')\n",
    "                )\n",
    "            .cast({'encounterId': pl.String})\n",
    "            .select('encounterId', 'admission_type', 'sapsii', 'sapsii_prob')\n",
    "            )\n",
    "    df_demo_extended = df_demo_extended.join(df_igs_clean, on='encounterId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_igs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset CHU : ajout des données présentes dans l'IGS (mtype d'admission, score SAPS, probabilité de survie)\n",
    "# Seul le premier IGS est gardé si plusieurs présents\n",
    "if DATASET == 'chu' : \n",
    "    df_igs_clean = (\n",
    "                df_igs  \n",
    "                .with_columns(\n",
    "                        pl.col('encounterNumber').map_elements(clean_encounter, return_dtype=pl.String).alias('encounterNumber'),\n",
    "                        pl.col('igsMort').map_elements(clean_percentage, return_dtype=pl.Float64).alias('sapsii_prob')\n",
    "                        )\n",
    "                .sort('encounterNumber', 'igsStoreTime')\n",
    "                .unique('encounterNumber', keep='first')\n",
    "                .with_columns(\n",
    "                    pl.when(pl.col('igsTypeAdm') == 0)\n",
    "                        .then(pl.lit('Medical'))\n",
    "                    .when(pl.col('igsTypeAdm') == 2)\n",
    "                        .then(pl.lit('Unscheduled Surgery'))\n",
    "                    .when(pl.col('igsTypeAdm') == 1)\n",
    "                        .then(pl.lit('Scheduled Surgery'))\n",
    "                    .otherwise(None).alias('admission_type')\n",
    "                    )\n",
    "                .rename(\n",
    "                    {\n",
    "                    'igsTotal' : 'sapsii',\n",
    "                    'igsGlgw' : 'score_glasgow'\n",
    "                    }\n",
    "                )\n",
    "                .select('encounterNumber', 'admission_type', 'sapsii', 'sapsii_prob', 'score_glasgow')\n",
    "                .cast(\n",
    "                    {'encounterNumber' : pl.Int64}, \n",
    "                    strict = False\n",
    "                )\n",
    "                )\n",
    "    \n",
    "    df_demo_extended = df_demo_extended.join(df_igs_clean, on='encounterNumber', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_igs_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_extended['unitLabel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_extended.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Admission type (from IGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudonymisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_extended.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des colonnes identifiantes\n",
    "col_identifiantes = [\n",
    "    'encounterNumber',\n",
    "    'lifeTimeNumber',\n",
    "    'lastName',\n",
    "    'firstName',\n",
    "    'dateOfBirth',\n",
    "    'cp',\n",
    "    'ville',\n",
    "    'adresse',\n",
    "    'utcInTime',\n",
    "    'utcOutTime',\n",
    "    'date_deces',\n",
    "    'code_lieu_deces',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrait des colonnes identifiantes\n",
    "# Attention dataset pseudonymisé non utilisable pour le traitement des séries temporelles car absence d'horodatage des entrées/sorties\n",
    "# => Uniquement pour analyse démogrpahique\n",
    "if DATASET == 'chu' : \n",
    "    df_indexed = df_demo_extended.with_row_index(offset=1)\n",
    "    table_corr = df_indexed.select(col_identifiantes)\n",
    "    df_pseudonymised = (\n",
    "        df_indexed\n",
    "            .with_columns(\n",
    "                year_inTime = pl.col('utcInTime').dt.year()\n",
    "            )\n",
    "            .select(\n",
    "                pl.exclude(col_identifiantes)\n",
    "                )\n",
    "        )\n",
    "    df_pseudonymised.head(5)\n",
    "    df_pseudonymised.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pseudonymised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_extended.write_parquet(OUTPUT_FOLDER + OUTPUT_STATIC_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_extended.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OUTPUT_FOLDER + OUTPUT_STATIC_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une table de correspondance pour le dataset CHU\n",
    "if DATASET == 'chu' : \n",
    "    table_corr.write_csv(OUTPUT_FOLDER + 'correlation_table.csv')\n",
    "    df_pseudonymised.write_parquet(OUTPUT_FOLDER + 'clean_pseudonimysed_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pseudonymised.head(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
