{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pré-traitement des fichiers \".parquet\" correspondant à des séries temporelles\n",
    "\n",
    "Dataset de sortie :  \n",
    "- une ligne par heure et par patient entre son entrée et sa sortie\n",
    "- une colonne par feature\n",
    "-  Nan si pas de valeur pour cette heure et ce patient\n",
    "- optionnel : 2e dataset censuré à 7 jours. Pour les patients sorties avant : Comblement avec Nan jusqu'à J7\n",
    "\n",
    "1. import des séries  \n",
    "2. Retrait des valeurs antérieurs à l'admission (offset de -1h en cas de bug à l'admission)  \n",
    "3. regroupement des données par heure \n",
    "    - pour les pressions artérielles : regroupement invasif et non invasif avec priorité sur l'invasif\n",
    "4. Ajout de NaN sur les heures manquantes (entre 1ère heure et dernière heure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_week_temporal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../params.json', 'r') as file :\n",
    "    params = json.load(file)\n",
    "\n",
    "DATASET, VERSION = params['dataset'], params['version']\n",
    "DATA_FOLD = params['data_folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes\n",
    "CENSUS_FILE = f'{DATA_FOLD}/{VERSION}/2.clean_data/{DATASET}/static/clean_static_encounters.parquet'\n",
    "INPUT_FOLDER = f'{DATA_FOLD}/{VERSION}/1.raw_data/{DATASET}/dynamic_features/'\n",
    "OUTPUT_FOLDER = f'{DATA_FOLD}/{VERSION}/2.clean_data/{DATASET}/temporal/'\n",
    "with open('temporal_range.json', 'r') as f:\n",
    "    temporal_range = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters = pl.read_parquet(CENSUS_FILE)\n",
    "encounters_list = encounters.select(pl.col('encounterId')).to_series().to_list()\n",
    "len(encounters_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Nettoyage du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pl.read_parquet(INPUT_FOLDER + 'pam_invasive.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleared_df(df, feature) :\n",
    "    feature_range = feature.replace('_non_invasive', '').replace('_invasive', '')\n",
    "    \n",
    "    print(feature)\n",
    "    if feature in ['pep', 'fio2'] : \n",
    "        lower_bound, upper_bound = temporal_range[feature_range]['range']\n",
    "    else : \n",
    "        # A ajouter Loop par patient \n",
    "        set_lower_bound, set_upper_bound = temporal_range[feature_range]['range']\n",
    "\n",
    "        df_ranged = df.filter(pl.col('valueNumber').is_between(set_lower_bound, set_upper_bound))\n",
    "\n",
    "        mean = df_ranged['valueNumber'].mean()\n",
    "        ds = df_ranged['valueNumber'].std()\n",
    "        stat_lower_bound = mean - (3 * ds)\n",
    "        stat_upper_bound = mean + 3*ds\n",
    "        print(f'{feature} : [{stat_lower_bound}-{stat_upper_bound}]')\n",
    "        \n",
    "        lower_bound = max(stat_lower_bound, set_lower_bound)\n",
    "        upper_bound = min(stat_upper_bound, set_upper_bound)\n",
    "    print(f'{lower_bound} / {upper_bound}')\n",
    "    df_cleared = (df\n",
    "        .select(\n",
    "                ['encounterId', 'delta_inTime_hours', 'valueNumber', 'feature']\n",
    "        )\n",
    "        # Retrait des données antérieurs à l'admission et des valeurs hors range\n",
    "        .filter(\n",
    "            pl.col('delta_inTime_hours') >= -1,\n",
    "            pl.col('valueNumber').is_between(lower_bound, upper_bound)\n",
    "        )\n",
    "        # Troncature de l'intervalle de la données par rapport à l'entrée             \n",
    "        .with_columns(\n",
    "            (pl.col(\"delta_inTime_hours\").cast(pl.Int64)).alias('intervalle')\n",
    "        )\n",
    "        \n",
    "        .group_by(\n",
    "            'encounterId', 'intervalle'\n",
    "        )\n",
    "        .agg(\n",
    "            pl.col(\"valueNumber\").median().alias(feature)\n",
    "        )\n",
    "        .sort(\n",
    "            'encounterId', 'intervalle'\n",
    "        )\n",
    "    )\n",
    "    print(df_cleared[feature].min())\n",
    "    print(df_cleared[feature].max())\n",
    "    return df_cleared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.3.2 Traitements des valeurs de pression artérielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressures_features = ['pam', 'pas', 'pad']\n",
    "non_invasive_pressures = [pressure +'_non_invasive' for pressure in pressures_features]\n",
    "invasive_pressures = [pressure +'_invasive' for pressure in pressures_features]\n",
    "\n",
    "# Fonction pour traiter les datasets de pressions\n",
    "def merged_pressure(invasive_df, non_invasive_df, feature_name):\n",
    "    # Lire les datasets invasifs et non invasifs\n",
    "\n",
    "    # Fusionner les datasets en priorisant les valeurs invasives\n",
    "    merged_df = (\n",
    "        invasive_df\n",
    "        .join(\n",
    "            non_invasive_df, on=[\"encounterId\", \"intervalle\"], how=\"full\"\n",
    "        )\n",
    "        .with_columns(\n",
    "            # Priorité aux valeurs invasives, compléter avec non-invasives si nécessaire\n",
    "            pl.coalesce([pl.col(f'{feature_name}_invasive'), pl.col(f'{feature_name}_non_invasive')]).alias(feature_name)\n",
    "        )\n",
    "        .with_columns(\n",
    "           pl.col(\"encounterId\").fill_null(pl.col(\"encounterId_right\")),\n",
    "           pl.col(\"intervalle\").fill_null(pl.col(\"intervalle_right\"))\n",
    "        )\n",
    "        .select(\n",
    "            ['encounterId', 'intervalle', feature_name]\n",
    "        )\n",
    "\n",
    "    )\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Comblement des intervalles manquants par Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_intervalle(df, feature) :\n",
    "    df_with_null = (\n",
    "        df\n",
    "        # Trouver l'intervalle maximum pour chaque patient\n",
    "        .group_by(\"encounterId\")\n",
    "        \n",
    "        .agg([\n",
    "            pl.col(\"intervalle\").max().alias(\"max_inter\")\n",
    "        ])\n",
    "\n",
    "        # Étendre l'intervalle pour inclure toutes les heures entre 0 et max_hour\n",
    "        .with_columns([\n",
    "            pl.struct(\n",
    "                [\"encounterId\", \"max_inter\"]\n",
    "                ).map_elements(\n",
    "                lambda row: list(range(0, row[\"max_inter\"] + 1)),\n",
    "                return_dtype=pl.List(pl.Int64)\n",
    "                ).alias(\"all_hours\")\n",
    "        ])\n",
    "\n",
    "        # Exploser les heures dans une nouvelle ligne\n",
    "        .explode(\"all_hours\")\n",
    "\n",
    "        # Joindre avec les données existantes pour aligner les heures\n",
    "        .join(\n",
    "            df,\n",
    "            left_on=[\"encounterId\", \"all_hours\"],\n",
    "            right_on=[\"encounterId\", \"intervalle\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Remplacer les valeurs manquantes par des NaN dans les colonnes de features\n",
    "\n",
    "        .with_columns([\n",
    "            pl.col(feature).fill_null(float('nan')) for feature in df.columns if feature not in [\"encounterId\", \"intervalle\"]\n",
    "        ])\n",
    "        \n",
    "        # Renommer les colonnes pour uniformité\n",
    "        .rename({\"all_hours\": \"intervalle\"})\n",
    "\n",
    "        # Réorganiser les colonnes\n",
    "        .select([\"encounterId\", \"intervalle\", feature])\n",
    "    )\n",
    "    return df_with_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(INPUT_FOLDER):\n",
    "\n",
    "    if filename.endswith('.parquet'):\n",
    "\n",
    "\n",
    "        feature = os.path.splitext(filename)[0]\n",
    "\n",
    "        if feature not in (non_invasive_pressures + invasive_pressures) :\n",
    "            \n",
    "            raw_df = pl.read_parquet(os.path.join(INPUT_FOLDER, filename))\n",
    "            cleared = cleared_df(raw_df, feature)\n",
    "            with_missing_values = fill_missing_intervalle(cleared, feature)\n",
    "            \n",
    "            with_missing_values.write_parquet(os.path.join(OUTPUT_FOLDER, f'cleared_{filename}'))\n",
    "            print(f'ok {feature}')\n",
    "            \n",
    "        \n",
    "        elif feature in invasive_pressures :\n",
    "            pressure_feature = feature.replace('_invasive', '')\n",
    "            non_invasive_feature = f'{pressure_feature}_non_invasive'\n",
    "            raw_invasive = pl.read_parquet(os.path.join(INPUT_FOLDER, filename))\n",
    "            \n",
    "            raw_non_invasive = pl.read_parquet(os.path.join(INPUT_FOLDER, filename.replace('invasive', 'non_invasive')))\n",
    "\n",
    "            cleared_invasive = cleared_df(raw_invasive, feature)\n",
    "            cleared_non_invasive = cleared_df(raw_non_invasive, non_invasive_feature)\n",
    "            merged_df = merged_pressure(cleared_invasive, cleared_non_invasive, pressure_feature)\n",
    "            with_missing_values = fill_missing_intervalle(merged_df, pressure_feature)\n",
    "\n",
    "            with_missing_values.write_parquet(os.path.join(OUTPUT_FOLDER, f'cleared_{pressure_feature}.parquet'))\n",
    "            print(f'ok {pressure_feature}')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge in 1 week dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_first_value(df) :\n",
    "    cleaned_dfs = []\n",
    "    features_col = list(set(df.columns) - set(['encounterId', 'intervalle']))\n",
    "    for encounterId, group in df.group_by(\"encounterId\"):\n",
    "        # Vérifie si au moins une variable n'est pas nulle\n",
    "        group = (group\n",
    "                    .fill_nan(None)\n",
    "                    .with_columns(\n",
    "                        pl.any_horizontal(features_col).is_not_null()\n",
    "                        .alias('has_data')\n",
    "                        )\n",
    "                )\n",
    "        # Trouver l'index de la première ligne où la variable has_data est True\n",
    "        first_valid_index = group.select(pl.col(\"has_data\")).to_pandas()[\"has_data\"].idxmax()\n",
    "\n",
    "        # Si aucun élément n'est valide, ignorer le patient\n",
    "        if first_valid_index == -1:\n",
    "            continue\n",
    "\n",
    "        # Garder les lignes à partir de la première valide\n",
    "        group = group[first_valid_index:]\n",
    "\n",
    "        # Réinitialiser l'intervalle pour commencer à zéro\n",
    "        group = group.with_columns(\n",
    "            (pl.col(\"intervalle\") - pl.lit(group[\"intervalle\"][0])).alias(\"intervalle\")\n",
    "        )\n",
    "        \n",
    "        # Supprimer la colonne temporaire \"has_data\"\n",
    "        group = group.drop(\"has_data\")\n",
    "\n",
    "\n",
    "        cleaned_dfs.append(group)\n",
    "\n",
    "    # Fusionner tous les groupes nettoyés\n",
    "    cleaned_df = pl.concat(cleaned_dfs)\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parquet_files(input_folder, static_df):\n",
    "    # Liste des `encounterId` uniques du dataset statique\n",
    "    encounter_ids = static_df[\"encounterId\"].unique()\n",
    "\n",
    "    # Création d'un dataframe de 0 à 180h \n",
    "    # 180h permet de prendre une marge de 12h supplémentaire afin de ne pas perdre de valeurs après suppression de l'offset\n",
    "    intervalle_series = pl.Series(\"intervalle\", range(0, 180))\n",
    "    intervalle_df = pl.DataFrame({\"intervalle\": intervalle_series})  \n",
    "      \n",
    "    # Créer l'intervalle standardisé de 0 à 180 heures pour chaque patient\n",
    "    standard_intervals = (\n",
    "        pl.DataFrame({\"encounterId\": encounter_ids})\n",
    "        .join(\n",
    "            intervalle_df,\n",
    "            how=\"cross\"  # Produit cartésien\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Contrôle du nombre de lignes du dataset des encounters\n",
    "    print(\"Standard intervals shape:\", standard_intervals.shape)\n",
    "\n",
    "    # Initialiser un dataframe standardisé\n",
    "    merged_df = standard_intervals\n",
    "\n",
    "    # Parcourir chaque fichier .parquet et le fusionner avec l'intervalle standardisé\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.endswith(\".parquet\"):\n",
    "            # Nom de la feature basée sur le nom du fichier\n",
    "            feature_name = os.path.splitext(file)[0]\n",
    "\n",
    "            # Charger le fichier .parquet\n",
    "            df = pl.read_parquet(os.path.join(input_folder, file))\n",
    "\n",
    "            # Joindre avec l'intervalle standardisé\n",
    "            merged_df = (\n",
    "                merged_df.join(\n",
    "                    df,\n",
    "                    on=[\"encounterId\", \"intervalle\"],\n",
    "                    how=\"left\"\n",
    "                )\n",
    "                .rename({feature_name: feature_name})  # Renommer pour conserver le nom original\n",
    "            )\n",
    "    merged_df = merged_df.drop(['pep', 'fio2'])\n",
    "    reset_intervalle_df = reset_first_value(merged_df)\n",
    "    \n",
    "    return reset_intervalle_df.filter(pl.col('intervalle') < 168)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected temporal dataset lenght : {encounters.unique('encounterId').shape[0]*168}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if one_week_temporal :\n",
    "    temporal_week = process_parquet_files(OUTPUT_FOLDER, encounters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6813180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_fold = f'{DATA_FOLD}/{VERSION}/3.analysis/times_series/{DATASET}/'\n",
    "temporal_week.write_parquet(temporal_fold + 'one_week.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_week['fr'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_encounters_folder = os.path.join(INPUT_FOLDER, 'missing_encounters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for filename in os.listdir(OUTPUT_FOLDER):\n",
    "    print(f'--------{filename}----------')\n",
    "    if filename.endswith(\".parquet\"):\n",
    "        # Charger le dataset .parquet\n",
    "        feature_data = pl.read_parquet(os.path.join(OUTPUT_FOLDER, filename))\n",
    "        \n",
    "        # Récupérer les encounterId du dataset .parquet\n",
    "        feature_encounters = feature_data.select(\"encounterId\").unique()\n",
    "        \n",
    "        # Trouver les lignes du dataset encounters n'apparaissant pas dans la feature\n",
    "        missing_encounters = encounters.join(feature_encounters, on=\"encounterId\", how=\"anti\")\n",
    "        \n",
    "        # Afficher le résultat\n",
    "        print(f\"Encounters manquants: {missing_encounters.shape[0]}/{encounters.unique('encounterId').shape[0]}\")\n",
    "        missing_filename = os.path.join(missing_encounters_folder, filename.replace('cleared', 'missing'))\n",
    "        #missing_encounters.write_parquet(missing_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = Path(INPUT_FOLDER)\n",
    "PARTITION_DIR = Path(OUTPUT_FOLDER + \"./tmp_by_patient\")\n",
    "PARTITION_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_FILE = \"tableau_fusionne.parquet\"\n",
    "SAMPLE_CSV = \"echantillon.csv\"\n",
    "LOG_FILE = \"log_erreurs.txt\"\n",
    "PIVOT_DIR = Path(OUTPUT_FOLDER + \"./tmp_feature_pivot\")\n",
    "PIVOT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FEATURES = [\n",
    "    \"nad_dose_poids\", \"tracheo\", \"ecmo_type\", \"poids_suivi\", \"pad_invasive\", \"endotracheal_tube\",\n",
    "    \"plq\", \"pplat\", \"spo2\", \"temp\", \"pam_invasive\", \"fio2\", \"urine_output\", \"tidal_volume\",\n",
    "    \"o2_flow\", \"pep\", \"fr\", \"admin_o2\", \"pad_non_invasive\", \"installation\", \"pas_invasive\",\n",
    "    \"mode_vent\", \"pas_non_invasive\", \"iv_input\", \"db_sang_cvvhf\", \"heart_rate\", \"pfc\",\n",
    "    \"pam_non_invasive\", \"cgr\", \"db_sang_hdi\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_FEATURES = [\n",
    "    \"tracheo\", \"ecmo_type\",\"endotracheal_tube\",\n",
    "     \"admin_o2\", \"installation\", \"mode_vent\"\n",
    "]\n",
    "\n",
    "NUM_FEATURES = [\n",
    "    \"nad_dose_poids\", \"poids_suivi\", \"pad_invasive\", \n",
    "    \"pplat\", \"spo2\", \"temp\", \"pam_invasive\",\"o2_flow\", \"fio2\", \"tidal_volume\",\n",
    "   \"pep\", \"fr\", \"pad_non_invasive\", \"pas_invasive\",\n",
    "    \"pas_non_invasive\", \"heart_rate\",\n",
    "    \"pam_non_invasive\", \"db_sang_cvvhf\", \"db_sang_hdi\"\n",
    "]\n",
    "SUM_FEATURES = [\n",
    "   \"urine_output\", \"iv_input\", \"plq\", \"pfc\",\"cgr\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_parquet(INPUT_FOLDER + 'mode_vent.parquet')['valueString'].value_counts().sort('count', descending=True).filter(pl.col('count') > 100)['valueString'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trach = pl.read_parquet(INPUT_FOLDER + 'tracheo.parquet')\n",
    "\n",
    "trach = (trach\n",
    "        .with_columns(\n",
    "            pl.any_horizontal([\n",
    "                pl.col('valueString').is_not_null(),\n",
    "                pl.col('utcValueDateTime').is_not_null(),\n",
    "                pl.col('valueNumber').is_not_null(),\n",
    "            ]).alias('has_trach')\n",
    "        )\n",
    "        .with_columns([\n",
    "            pl.col('has_trach').cast(pl.String).alias('valueString'),\n",
    "            pl.lit(None, dtype=pl.Datetime).alias('utcValueDateTime'),\n",
    "            pl.lit(None, dtype=pl.Float64).alias('valueNumber'),\n",
    "    ])\n",
    "    # Si vous ne souhaitez plus garder la colonne d'origine\n",
    "    .drop('has_trach')\n",
    "    .filter(pl.col('valueString').is_not_null())\n",
    "\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iot = pl.read_parquet(INPUT_FOLDER + 'endotracheal_tube.parquet')\n",
    "print(iot.columns)\n",
    "iot = (iot\n",
    "        .with_columns(\n",
    "            pl.any_horizontal([\n",
    "                pl.col('valueString').is_not_null(),\n",
    "                pl.col('utcValueDateTime').is_not_null(),\n",
    "                pl.col('valueNumber').is_not_null(),\n",
    "            ]).alias('has_ett')\n",
    "        )\n",
    "        .with_columns([\n",
    "            pl.col('has_ett').cast(pl.String).alias('valueString'),\n",
    "            pl.lit(None, dtype=pl.Datetime).alias('utcValueDateTime'),\n",
    "            pl.lit(None, dtype=pl.Float64).alias('valueNumber'),\n",
    "    ])\n",
    "    # Si vous ne souhaitez plus garder la colonne d'origine\n",
    "    .drop('has_ett')\n",
    "    .filter(pl.col('valueString').is_not_null())\n",
    "\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install = pl.read_parquet(INPUT_FOLDER + 'installation.parquet') \n",
    "\n",
    "install = install.with_columns(\n",
    "    pl.when(\n",
    "        pl.col('valueString').str.contains('Ventral', literal=True)\n",
    "        ).then(pl.lit('Ventral'))\n",
    "        .otherwise(pl.lit(None, dtype=pl.String))\n",
    "        .alias('valueString'),\n",
    "    pl.lit(None, dtype=pl.Float64).alias('valueNumber')\n",
    ").filter(pl.col('valueString').is_not_null())\n",
    "\n",
    "install['valueString'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgr = pl.read_parquet(INPUT_FOLDER + 'cgr.parquet') \n",
    "\n",
    "cgr = cgr.with_columns(\n",
    "    pl.when(pl.col('valueString').str.contains('CGR', literal=True))\n",
    "    .then(True).otherwise(pl.lit(None)).cast(pl.Float64).alias('valueNumber')\n",
    "    ,pl.lit(None, dtype=pl.Float64).alias('valueString')\n",
    "    ).filter(pl.col('valueNumber').is_not_null())\n",
    "\n",
    "plq = pl.read_parquet(INPUT_FOLDER + 'plq.parquet') \n",
    "\n",
    "plq = plq.with_columns(\n",
    "    pl.when(pl.col('valueString').str.contains('PLAQUETTE', literal=True))\n",
    "    .then(True).otherwise(pl.lit(None)).cast(pl.Float64).alias('valueNumber')\n",
    "    ,pl.lit(None, dtype=pl.Float64).alias('valueString')\n",
    "    ).filter(pl.col('valueNumber').is_not_null())\n",
    "\n",
    "pfc = pl.read_parquet(INPUT_FOLDER + 'pfc.parquet') \n",
    "\n",
    "pfc = pfc.with_columns(\n",
    "    pl.when(pl.col('valueString').str.contains('PFC', literal=True))\n",
    "    .then(True).otherwise(pl.lit(None)).cast(pl.Float64).alias('valueNumber')\n",
    "    ,pl.lit(None, dtype=pl.Float64).alias('valueString')\n",
    "    ).filter(pl.col('valueNumber').is_not_null())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_files = sorted(INPUT_DIR.glob(\"*.parquet\"))\n",
    "col_order = ['encounterId', 'ptCensusId', 'feature', 'valueString', 'valueNumber', 'utcValueDateTime', 'utcChartTime', 'utcInTime', 'delta_inTime_hours']\n",
    "\n",
    "feature_dfs = {}\n",
    "log_lines = []\n",
    "\n",
    "for file in tqdm(feature_files, desc=\"Préchargement des features\"):\n",
    "    try:\n",
    "        if str(file).endswith(\"endotracheal_tube.parquet\"):\n",
    "            df = iot.filter(pl.col(\"encounterId\").is_in(encounters_list))\n",
    "        elif str(file).endswith(\"tracheo.parquet\"):\n",
    "            df = trach.filter(pl.col(\"encounterId\").is_in(encounters_list))\n",
    "        elif str(file).endswith(\"installation.parquet\"):\n",
    "            df = install.filter(pl.col(\"encounterId\").is_in(encounters_list))\n",
    "        elif str(file).endswith(\"cgr.parquet\"):\n",
    "            df = cgr.filter(pl.col(\"encounterId\").is_in(encounters_list))\n",
    "        elif str(file).endswith(\"pfc.parquet\"):\n",
    "            df = pfc.filter(pl.col(\"encounterId\").is_in(encounters_list))\n",
    "        elif str(file).endswith(\"plq.parquet\"):\n",
    "            df = plq.filter(pl.col(\"encounterId\").is_in(encounters_list))\n",
    "        else :\n",
    "            df = pl.read_parquet(file)\n",
    "            df = df.filter(pl.col(\"encounterId\").is_in(encounters_list))\n",
    "        if str(file).endswith(\"urine_output.parquet\") or str(file).endswith(\"iv_input.parquet\"):\n",
    "            df = df.with_columns([\n",
    "                pl.lit(None, dtype=pl.Utf8).alias(\"valueString\"),\n",
    "                pl.lit(None, dtype=pl.Datetime).alias(\"utcValueDateTime\")\n",
    "            ]).select(col_order).filter(pl.col(\"encounterId\").is_in(encounters_list))\n",
    "        \n",
    "        feature_dfs[file.name] = df.drop('utcValueDateTime', 'ptCensusId', 'utcInTime', 'delta_inTime_hours')\n",
    "    except Exception as e:\n",
    "        log_lines.append(f\"Erreur dans le fichier {file.name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pivoted_df = {}\n",
    "for feature_name, df in feature_dfs.items():\n",
    "    feat = feature_name.replace('.parquet', '')\n",
    "    if feat in STR_FEATURES:\n",
    "        print(f'str : {feat}')\n",
    "        df_pivot = (df\n",
    "                .select(\n",
    "                        ['encounterId', 'utcChartTime', 'feature', 'valueString']\n",
    "                    )\n",
    "                .sort(\n",
    "                        'encounterId', 'utcChartTime'\n",
    "                    )\n",
    "                .pivot(\n",
    "                        index=['encounterId', 'utcChartTime'],\n",
    "                        on='feature',\n",
    "                        values='valueString',\n",
    "                        aggregate_function='first'\n",
    "                    )\n",
    "                .cast(\n",
    "                        {'encounterId': pl.Int32}\n",
    "                )\n",
    "        )\n",
    "    elif feat in NUM_FEATURES:\n",
    "        print(f'num : {feat}')\n",
    "        df_pivot = (df\n",
    "                .select(\n",
    "                        ['encounterId', 'utcChartTime', 'feature', 'valueNumber']\n",
    "                    )\n",
    "                .sort(\n",
    "                        'encounterId', 'utcChartTime'\n",
    "                    )\n",
    "                .pivot(\n",
    "                        index=['encounterId', 'utcChartTime'],\n",
    "                        on='feature',\n",
    "                        values='valueNumber',\n",
    "                        aggregate_function='median'\n",
    "                    )\n",
    "                .cast(\n",
    "                        {'encounterId': pl.Int32}\n",
    "                )\n",
    "            )\n",
    "    elif feat in SUM_FEATURES:\n",
    "        print(f'sum : {feat}')\n",
    "        df_pivot = (df\n",
    "                .select(\n",
    "                        ['encounterId', 'utcChartTime', 'feature', 'valueNumber']\n",
    "                    )\n",
    "                .sort(\n",
    "                        'encounterId', 'utcChartTime'\n",
    "                    )\n",
    "                .pivot(\n",
    "                        index=['encounterId', 'utcChartTime'],\n",
    "                        on='feature',\n",
    "                        values='valueNumber',\n",
    "                        aggregate_function='sum'\n",
    "                    )\n",
    "                .cast(\n",
    "                        {'encounterId': pl.Int32}\n",
    "                )\n",
    "            )\n",
    "    df_pivot.write_parquet(PIVOT_DIR / f'{feat}.parquet')\n",
    "    pivoted_df[feat] = df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join des features pivotées sur l'encounterId et l'utcChartTime\n",
    "\n",
    "# Sauvegarde des fichiers pris en compte si bug pendant la procédure\n",
    "done_files = []\n",
    "\n",
    "joined_df = pl.read_parquet(PIVOT_DIR / 'heart_rate.parquet')\n",
    "\n",
    "\n",
    "for file in tqdm(PIVOT_DIR.glob(\"*.parquet\"), desc=\"Scan des features\"):\n",
    "    print(f'Début : {str(file).split('/')[-1]}')\n",
    "    df = pl.read_parquet(file)\n",
    "    if not str(file).endswith('heart_rate.parquet') and file not in done_files:\n",
    "        joined_df = joined_df.join(\n",
    "            df,\n",
    "            on=['encounterId', 'utcChartTime'],\n",
    "            how='full',\n",
    "            coalesce=True\n",
    "        )\n",
    "        print('jointure ok')\n",
    "    joined_df.write_parquet(OUTPUT_FILE)\n",
    "    done_files.append(file)\n",
    "    print(f'fichier terminé')\n",
    "\n",
    "joined_df = joined_df.sort('encounterId', 'utcChartTime')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.write_parquet(OUTPUT_FOLDER + OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by 1 hour dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_chart = joined_df.select('encounterId', 'utcChartTime').group_by('encounterId').agg(pl.col('utcChartTime').min().name.suffix('_min'))\n",
    "\n",
    "joined_delta = (\n",
    "    joined_df\n",
    "        .join(min_chart, on ='encounterId', how='left')\n",
    "        .with_columns(\n",
    "            ((pl.col(\"utcChartTime\") - pl.col(\"utcChartTime_min\")).dt.total_minutes() / 60).cast(pl.Int32).alias(\"delta_hour\")\n",
    "        )\n",
    "        .group_by(['encounterId', 'delta_hour'])\n",
    "        .agg(\n",
    "            [\n",
    "                pl.first(STR_FEATURES),\n",
    "                pl.median(NUM_FEATURES),\n",
    "                pl.sum(SUM_FEATURES)\n",
    "            ]\n",
    "        )\n",
    "        .select(['encounterId', 'delta_hour'] + STR_FEATURES + NUM_FEATURES + SUM_FEATURES)\n",
    "        .sort('encounterId', 'delta_hour')\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_delta.filter(pl.col())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_delta.sort('encounterId', 'delta_hour').write_parquet(OUTPUT_FOLDER + 'all_features_with_delta.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(OUTPUT_FOLDER + 'all_features_with_delta.parquet')\n",
    "df_encounters = df.filter(pl.col('delta_hour') > 24 )['encounterId'].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "rand_encounter = random.choice(df_encounters)\n",
    "df_encounter = df.filter(pl.col('encounterId') == rand_encounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encounter.with_columns(\n",
    "            pl.when(pl.col(SUM_FEATURES) > 0).then(pl.col(SUM_FEATURES)).otherwise(pl.lit(None, dtype=pl.Float64))\n",
    "        ).filter(~pl.all_horizontal(pl.col(ALL_FEATURES).is_null()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "variables = NUM_FEATURES  # Remplacez par les colonnes de df\n",
    "df_encounter = df_encounter.select([\"delta_hour\"] + variables).to_pandas()\n",
    "\n",
    "# Créer une figure avec plusieurs sous-graphiques (un par variable)\n",
    "fig, axes = plt.subplots(len(variables), 1, figsize=(10, 5 * len(variables)), sharex=True)\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    if df_encounter[var].sum() > 0 :\n",
    "        sns.lineplot(data=df_encounter, x=\"delta_hour\", y=var, ax=axes[i])\n",
    "        axes[i].set_title(f\"Évolution de {var} en fonction de delta_hour\")\n",
    "        axes[i].set_ylabel(var)\n",
    "\n",
    "# Ajouter un label commun pour l'axe des x\n",
    "plt.xlabel(\"delta_hour\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
