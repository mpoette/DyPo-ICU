{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../params.json', 'r') as file :\n",
    "    params = json.load(file)\n",
    "\n",
    "DATASET, VERSION, DATA_FOLD = params['dataset'], params['version'], params['data_folder']\n",
    "\n",
    "print(f'Working on {DATASET} dataset {VERSION}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(f'{DATA_FOLD}/{VERSION}/3.analysis/imputation_48/{DATASET}/first_48h.parquet').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# tqdm sur la boucle principale\n",
    "for n in tqdm(range(1, 49), desc=\"Calcul en cours\", unit=\"taille\"):\n",
    "    count = 0\n",
    "\n",
    "    for _, group in df.groupby('encounterId'):\n",
    "        group_sorted = group.sort_values('intervalle')\n",
    "        complete = (group_sorted['total_missing'] == 0).astype(int).to_numpy()\n",
    "        \n",
    "        rolling_sums = pd.Series(complete).rolling(window=n).sum()\n",
    "        count += (rolling_sums == n).sum()\n",
    "\n",
    "    results[n] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Si ce n'est pas déjà fait : results = {n: count} calculé auparavant\n",
    "\n",
    "# Extraire les valeurs pour le graphique\n",
    "n_values = list(results.keys())\n",
    "interval_counts = list(results.values())\n",
    "\n",
    "# Tracer le graphique\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_values, interval_counts, marker='o', linestyle='-', color='royalblue')\n",
    "#plt.title(\"Nombre d'intervalles complets selon la taille de la fenêtre\", fontsize=14)\n",
    "plt.xlabel(\"Taille de l'intervalle (n timestamps consécutifs)\", fontsize=12)\n",
    "plt.ylabel(\"Nombre d'intervalles complets\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.xticks(range(0, 49, 2))  # pour lisibilité\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset séries temporelles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit des séries de 9 timestamps sans valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['heart_rate', 'spo2', 'fr', 'pam']\n",
    "\n",
    "df_sorted = df.sort_values(by=['encounterId', 'intervalle'])\n",
    "\n",
    "windows_list = []  \n",
    "ts_ids = []       \n",
    "ts_counter = 0\n",
    "\n",
    "# Parcourir chaque groupe d'éncounters avec une barre de progression\n",
    "for encounter_id, group in tqdm(df_sorted.groupby('encounterId'), desc=\"Processing encounters\"):\n",
    "    group_sorted = group.sort_values('intervalle').reset_index(drop=True)\n",
    "    \n",
    "    # Conversion en array NumPy\n",
    "    group_array = group_sorted[features].to_numpy()\n",
    "    n = group_array.shape[0]\n",
    "    \n",
    "    # fenêtre de 9 timestamps\n",
    "    for i in range(n - 8):\n",
    "        window = group_array[i:i+9]  # Fenêtre de taille 9 x len(features)\n",
    "        # Vérification de la présence de NaN\n",
    "        if np.isnan(window).any():\n",
    "            continue\n",
    "        windows_list.append(window)\n",
    "        ts_ids.append(ts_counter)  \n",
    "        ts_counter += 1\n",
    "\n",
    "array_3d = np.stack(windows_list, axis=0)\n",
    "print(\"Nombre de séries temporelles retenues :\", array_3d.shape[0])\n",
    "print(\"Dimensions de X :\", array_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'{DATA_FOLD}/{VERSION}/3.analysis/outliers/{DATASET}/datasets'\n",
    "np.save(save_path, array_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_df = scaler.fit_transform(array_3d.reshape(-1,4)).reshape(-1,9,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random_idx = np.random.randint(array_3d.shape[0], size=30000)\n",
    "learning_idx, test_idx = random_idx[:20000], random_idx[20000:]\n",
    "learning_set, test_set = scaled_df[learning_idx,:,:], scaled_df[test_idx,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = learning_set.copy()\n",
    "X_masked = test_set.copy()\n",
    "X_masked[:, 4, :] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypots.imputation import SAITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits = SAITS(\n",
    "    n_steps=9,\n",
    "    n_features=4,\n",
    "    n_layers=3, d_model=512, d_ffn=128, n_heads=8, d_k=64, d_v=64,\n",
    "    dropout=0.1,\n",
    "    device=\"cuda\",  # change to \"cuda\" si GPU dispo,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits.fit({'X' : X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits.save(f'{DATA_FOLD}/{VERSION}/4.models/outliers/{DATASET}/windows_9_timestamps.pypots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imputed = saits.impute({'X':X_masked})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_per_sample = np.mean((test_set[:,4,:] - X_imputed[:,4,:])**2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_per_sample.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "inertias = []\n",
    "range_n = range(2, 10)\n",
    "\n",
    "for k in range_n:\n",
    "    km = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\", random_state=42)\n",
    "    km.fit(test_set)\n",
    "    inertias.append(km.inertia_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range_n, inertias, marker='o')\n",
    "plt.title(\"Méthode du coude (DTW Inertia)\")\n",
    "plt.xlabel(\"Nombre de clusters\")\n",
    "plt.ylabel(\"Inertie (DTW)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cluster = TimeSeriesKMeans(n_clusters=4, metric=\"dtw\", random_state=0)\n",
    "clusters = model_cluster.fit_predict(X_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x=clusters, y=mse_per_sample)\n",
    "plt.title(\"Erreur d'imputation (MSE) vs Forme de la série temporelle\")\n",
    "plt.xlabel(\"Cluster de forme (DTW)\")\n",
    "plt.ylabel(\"MSE de l'imputation\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# 8. (Optionnel) Visualisation des prototypes de chaque cluster\n",
    "# --------------------\n",
    "for i in range(4):\n",
    "    plt.figure()\n",
    "    plt.title(f\"Prototype du cluster {i}\")\n",
    "    for f in range(4):\n",
    "        plt.plot(km.cluster_centers_[i][:, f], label=f\"Signal {f+1}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
